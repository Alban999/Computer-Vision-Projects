{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b47b15de-64a5-4fa9-a688-23d3efa9a2f4","_uuid":"0cc385a7-98f6-4883-96eb-7b89c7c9aa1c","id":"7400e183","papermill":{"duration":0.016533,"end_time":"2022-04-12T14:48:23.471825","exception":false,"start_time":"2022-04-12T14:48:23.455292","status":"completed"},"tags":[]},"source":["<div style=\"width:100%; height:140px\">\n","    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n","</div>\n","\n","\n","KUL H02A5a Computer Vision: Group Assignment 2\n","---------------------------------------------------------------\n","Student numbers: <span style=\"color:red\">r1, r2, r3, r4, r5</span>. (fill in your student numbers!)\n","\n","In this group assignment your team will delve into some deep learning applications for computer vision. The assignment will be delivered in the same groups from *Group assignment 1* and you start from this template notebook. The notebook you submit for grading is the last notebook you submit in the [Kaggle competition](https://www.kaggle.com/t/d11be6a431b84198bc85f54ae7e2563f) prior to the deadline on **Tuesday 24 May 23:59**. Closely follow [these instructions](https://github.com/gourie/kaggle_inclass) for joining the competition, sharing your notebook with the TAs and making a valid notebook submission to the competition. A notebook submission not only produces a *submission.csv* file that is used to calculate your competition score, it also runs the entire notebook and saves its output as if it were a report. This way it becomes an all-in-one-place document for the TAs to review. As such, please make sure that your final submission notebook is self-contained and fully documented (e.g. provide strong arguments for the design choices that you make). Most likely, this notebook format is not appropriate to run all your experiments at submission time (e.g. the training of CNNs is a memory hungry and time consuming process; due to limited Kaggle resources). It can be a good idea to distribute your code otherwise and only summarize your findings, together with your final predictions, in the submission notebook. For example, you can substitute experiments with some text and figures that you have produced \"offline\" (e.g. learning curves and results on your internal validation set or even the test set for different architectures, pre-processing pipelines, etc). We advise you to first go through the PDF of this assignment entirely before you really start. Then, it can be a good idea to go through this notebook and use it as your first notebook submission to the competition. You can make use of the *Group assignment 2* forum/discussion board on Toledo if you have any questions. Good luck and have fun!\n","\n","---------------------------------------------------------------\n","NOTES:\n","* This notebook is just a template. Please keep the five main sections, but feel free to adjust further in any way you please!\n","* Clearly indicate the improvements that you make! You can for instance use subsections like: *3.1. Improvement: applying loss function f instead of g*.\n"]},{"cell_type":"markdown","metadata":{"_cell_guid":"35358cfb-b13d-4277-8dd5-4e663c8cd775","_uuid":"3b40b846-d7da-46d8-b354-c6d5c5ded56e","id":"d27e4eb4","papermill":{"duration":0.014397,"end_time":"2022-04-12T14:48:23.501501","exception":false,"start_time":"2022-04-12T14:48:23.487104","status":"completed"},"tags":[]},"source":["# 1. Overview\n","This assignment consists of *three main parts* for which we expect you to provide code and extensive documentation in the notebook:\n","* Image classification (Sect. 2)\n","* Semantic segmentation (Sect. 3)\n","* Adversarial attacks (Sect. 4)\n","\n","In the first part, you will train an end-to-end neural network for image classification. In the second part, you will do the same for semantic segmentation. For these two tasks we expect you to put a significant effort into optimizing performance and as such competing with fellow students via the Kaggle competition. In the third part, you will try to find and exploit the weaknesses of your classification and/or segmentation network. For the latter there is no competition format, but we do expect you to put significant effort in achieving good performance on the self-posed goal for that part. Finally, we ask you to reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Sect. 5). It is important to note that only a small part of the grade will reflect the actual performance of your networks. However, we do expect all things to work! In general, we will evaluate the correctness of your approach and your understanding of what you have done that you demonstrate in the descriptions and discussions in the final notebook."]},{"cell_type":"markdown","metadata":{"id":"96360d34","papermill":{"duration":0.014263,"end_time":"2022-04-12T14:48:23.530341","exception":false,"start_time":"2022-04-12T14:48:23.516078","status":"completed"},"tags":[]},"source":["## 1.1 Deep learning resources\n","If you did not yet explore this in *Group assignment 1 (Sect. 2)*, we recommend using the TensorFlow and/or Keras library for building deep learning models. You can find a nice crash course [here](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO)."]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"7ddf657a-b938-4a49-87dc-b0db9af9156d","_uuid":"c65ea4f1-cc90-408f-b8e0-7c7399ec7e21","execution":{"iopub.execute_input":"2022-05-20T17:50:59.447758Z","iopub.status.busy":"2022-05-20T17:50:59.447389Z","iopub.status.idle":"2022-05-20T17:51:05.885685Z","shell.execute_reply":"2022-05-20T17:51:05.884945Z","shell.execute_reply.started":"2022-05-20T17:50:59.447674Z"},"id":"79973719","papermill":{"duration":5.416492,"end_time":"2022-04-12T14:48:28.96151","exception":false,"start_time":"2022-04-12T14:48:23.545018","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from keras import backend as bck\n","from sklearn.utils.class_weight import compute_class_weight\n","from keras.preprocessing import image  \n","from keras.layers import Activation, MaxPooling2D, BatchNormalization, Conv2D, Dense, GlobalAveragePooling2D, Dropout, Flatten\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import Sequential, Model\n","from keras.preprocessing.image import ImageDataGenerator\n","import cv2\n","from sklearn.model_selection import train_test_split\n","import random"]},{"cell_type":"markdown","metadata":{"id":"0f890505","papermill":{"duration":0.014416,"end_time":"2022-04-12T14:48:28.990998","exception":false,"start_time":"2022-04-12T14:48:28.976582","status":"completed"},"tags":[]},"source":["## 1.2 PASCAL VOC 2009\n","For this project you will be using the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. This dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes."]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"1ce67f49-6bf6-4e5c-b5e4-576e893616a9","_uuid":"3b1c5fbb-757f-4349-b224-e281c540e1ad","execution":{"iopub.execute_input":"2022-05-20T17:51:10.885187Z","iopub.status.busy":"2022-05-20T17:51:10.884648Z","iopub.status.idle":"2022-05-20T17:51:30.698266Z","shell.execute_reply":"2022-05-20T17:51:30.697440Z","shell.execute_reply.started":"2022-05-20T17:51:10.885151Z"},"id":"bd4b124d","outputId":"c1f8c872-e409-4ffc-b372-a50ea799769f","papermill":{"duration":21.336481,"end_time":"2022-04-12T14:48:50.342062","exception":false,"start_time":"2022-04-12T14:48:29.005581","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Loading the training data\n","# train_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2022/train/train_set.csv', index_col=\"Id\")\n","train_df = pd.read_csv('../input/kul-h02a5a-computer-vision-ga2-2022/train/train_set.csv', index_col=\"Id\")\n","\n","labels = train_df.columns\n","train_df[\"img\"] = [np.load('../input/kul-h02a5a-computer-vision-ga2-2022/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n","train_df[\"seg\"] = [np.load('../input/kul-h02a5a-computer-vision-ga2-2022/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n","print(\"The training set contains {} examples.\".format(len(train_df)))\n","\n","# Show some examples\n","fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n","for i, label in enumerate(labels):\n","    df = train_df.loc[train_df[label] == 1]\n","    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n","    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n","    axs[0, i].axis(\"off\")\n","    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n","    axs[1, i].axis(\"off\")\n","    \n","plt.show()\n","\n","# The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\n","train_df.head(1)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T17:51:48.325917Z","iopub.status.busy":"2022-05-20T17:51:48.325289Z","iopub.status.idle":"2022-05-20T17:51:48.331495Z","shell.execute_reply":"2022-05-20T17:51:48.330576Z","shell.execute_reply.started":"2022-05-20T17:51:48.325877Z"},"id":"JQf7Is5wdygV","outputId":"042cfd07-5837-4ea3-8b7e-099692f2dd2c","trusted":true},"outputs":[],"source":["train_size = train_df.index.size\n","train_size"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T17:49:39.870543Z","iopub.status.busy":"2022-05-20T17:49:39.868997Z","iopub.status.idle":"2022-05-20T17:49:39.887008Z","shell.execute_reply":"2022-05-20T17:49:39.886246Z","shell.execute_reply.started":"2022-05-20T17:49:39.870488Z"},"id":"_gsPOeHcbjD1","outputId":"8f977c36-54b1-4390-f6e6-63ffb4cb4f0c","trusted":true},"outputs":[],"source":["labels = train_df.columns[:20].tolist()\n","labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T17:17:45.233412Z","iopub.status.busy":"2022-05-20T17:17:45.232785Z","iopub.status.idle":"2022-05-20T17:17:53.114511Z","shell.execute_reply":"2022-05-20T17:17:53.112705Z","shell.execute_reply.started":"2022-05-20T17:17:45.233363Z"},"trusted":true},"outputs":[],"source":["# Loading the test data\n","test_df = pd.read_csv('../input/kul-h02a5a-computer-vision-ga2-2022/test/test_set.csv', index_col=\"Id\")\n","test_df[\"img\"] = [np.load('../input/kul-h02a5a-computer-vision-ga2-2022/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n","test_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\n","print(\"The test set contains {} examples.\".format(len(test_df)))\n","\n","# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\n","test_df.head(1)\n","test_size = test_df.index.size"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset analysis"]},{"cell_type":"markdown","metadata":{},"source":["To have a good classification, we need that our dataset is balance, i.e. each class occurence has to be roughly equal. In this case, we say that the dataset is balanced."]},{"cell_type":"markdown","metadata":{},"source":["Let us analyze the dataset. Below is the reparition of the class."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["counts = np.zeros(20)\n","\n","for i, el in enumerate(train_df.iloc):\n","    for j,lab in enumerate(labels):\n","        counts[j] += train_df.iloc[i][lab]\n","\n","fig, ax = plt.subplots(figsize=(15, 10))\n","ax.barh(sorted(labels), counts, align='center')\n","ax.set_title('Counts')\n","ax.xaxis.tick_top()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can notice that our dataset is unbalanced. As previously said, for a good classification, we need a balanced dataset. Several methods exist to transform our dataset in a balanced one: upsampling (increasing occurences of the minority classes), downsampling (decreasing the occurecens of the majority classes), use weights (set a high weight for minority classes and low of the others).\n","As we want that our neural network learn a general behaviour, we will not use upsampling and downsampling. Upsampling will set too much importance on certain images by duplicating these in the dataset. Downsampling is also not a good idea as we could lose certain information (we could have a minority and a majority class in the same picture, thus deleting this picture will make us lose information). "]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["The preprocessing part is composed of several steps: resize + normalization + shuffle of the data"]},{"cell_type":"markdown","metadata":{},"source":["Here we make the choice of resizing the images to 64x64 and not 128x128 so that the CNN learns faster"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["im_size = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OhLTZK7mdPxi","trusted":true},"outputs":[],"source":["def preprocess(img, im_size):\n","\n","  # Resize + normalize\n","  \n","  img = cv2.resize(img, dsize=(im_size, im_size), interpolation=cv2.INTER_LINEAR)\n","  img = img/255.0\n","  \n","  return img.astype('float32')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LpZTe9Qpdbqg","trusted":true},"outputs":[],"source":["processed_train_data_images = np.zeros((train_size, im_size,im_size,3))\n","processed_test_data_images = np.zeros((test_size, im_size,im_size,3))\n","\n","for i in range(train_size):\n","  processed_train_data_images[i] = preprocess(train_df.loc[i]['img'], im_size)\n","\n","for i in range(train_size):\n","  processed_test_data_images[i] = preprocess(test_df.loc[i]['img'], im_size)\n","\n","processed_train_data_labels = np.array(train_df.drop(['img', 'seg'], axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfqBIijtt-j7","trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(processed_train_data_images, processed_train_data_labels, test_size=0.33)"]},{"cell_type":"markdown","metadata":{"id":"Bs8IxNUbvVRs","outputId":"1ca005af-c6e0-43ec-fb86-6e355808b81c"},"source":["Let us now shuffle the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSLwHN8uvaZn","outputId":"bf5a3085-d8c2-4eae-c21c-cf2349c83541","trusted":true},"outputs":[],"source":["  random.Random(30).shuffle(X_train)\n","  random.Random(30).shuffle(y_train)"]},{"cell_type":"markdown","metadata":{},"source":["## Classification"]},{"cell_type":"markdown","metadata":{},"source":["Now that our dataset is preprocessed, let us now jump into the classification"]},{"cell_type":"markdown","metadata":{},"source":["First, let us solve the unbalanced dataset problem. To do so, we will put weights for every classes\n","(we avoid that the model thinks it has learned by predicting the majority class only)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def weights_calculator(y):\n","  w = np.empty([20, 2])\n","  for i in range(20):\n","    w[i] = compute_class_weight( class_weight = \"balanced\", classes = [0.,1.], y = y[:,i] )\n","  return w\n","\n","class_weights = weights_calculator(y_train)\n","class_weights"]},{"cell_type":"markdown","metadata":{},"source":["## Accuracy metrics"]},{"cell_type":"markdown","metadata":{},"source":["An other important step is the choice of the metric we will used to determine the classification accuracy. As we are dealing with a multi-label classificiation problem, the metric used is the fbeta function (with beta = 1.0)."]},{"cell_type":"markdown","metadata":{},"source":["The fbeta function takes into account:\n","- The precision quantifies the number of correct true positives predicted: $Prec = \\frac{TP}{(TP+FP)}$\n","- The recall quantifies the number of correct positives out of all the positive predictions which has been made: $Rec = \\frac{TP}{(TP+FN)}$"]},{"cell_type":"markdown","metadata":{},"source":["By maximizing the precision, we minimize FP and by maximizing recall, we minimize FN."]},{"cell_type":"markdown","metadata":{},"source":["The fbeta function combines both precision and recall via the following formula:\n","$F_{\\beta} = \\frac{(1+\\beta^2)*Prec*Rec}{beta^2*Prec+Rec}$"]},{"cell_type":"markdown","metadata":{},"source":["Here we will take $\\beta$ = 1 to set an even importance on the precision and the recall"]},{"cell_type":"markdown","metadata":{},"source":["Sklearn proposes a $F_\\beta$ function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import fbeta_score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fbeta_score([1,0,0,1],[1,0,0,1],beta=0)"]},{"cell_type":"markdown","metadata":{},"source":["Here we propose one calculated by hand"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Based on https://github.com/kenanEkici/multilabel-class-pascalvoc\n","def f_beta_1(y, y_pred):\n","  true_positives = bck.sum(bck.round(bck.clip(y * y_pred, 0, 1)))\n","  possible_positives = bck.sum(bck.round(bck.clip(y, 0, 1)))\n","  predicted_positives = bck.sum(bck.round(bck.clip(y_pred, 0, 1)))\n","  precision = true_positives / (predicted_positives + bck.epsilon())\n","  recall = true_positives / (possible_positives + bck.epsilon())\n","  f_beta_1 = 2*(precision*recall)/(precision+recall + bck.epsilon())\n","  return f_beta_1"]},{"cell_type":"markdown","metadata":{},"source":["## Loss function"]},{"cell_type":"markdown","metadata":{},"source":["Now we have to chose a loss function which corresponds to our multi-label classification problem."]},{"cell_type":"markdown","metadata":{},"source":["Here we propose a Binary Cross Entropy loss function because it includes the cross entropy, the sigmoid function (we cannot use softmax as we can have multiple classed for one instance/picture) and it forces the output to be 0 or 1."]},{"cell_type":"markdown","metadata":{},"source":["This loss function calculates the error between the prediction and the ground truth label."]},{"cell_type":"markdown","metadata":{},"source":["The choice of the sigmoid function for a multi-label classification problem is very important. Sigmoid allows to have N independent binary classification problems (in opposition with Softmax)."]},{"cell_type":"markdown","metadata":{},"source":["As we have to take into account the weight of each class (to deal with our unbalanced dataset), we define a new loss function which combines the binary cross entropy behaviour and the weights for each class."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Based on https://github.com/kenanEkici/multilabel-class-pascalvoc\n","def loss_binary_weights(w):\n","      def weighted_loss(y, y_pred):\n","        return bck.mean((w[:,0]**(1-y))*(w[:,1]**(y))*bck.binary_crossentropy(y, y_pred), axis=-1)\n","      return weighted_loss"]},{"cell_type":"markdown","metadata":{},"source":["## Neural network"]},{"cell_type":"markdown","metadata":{},"source":["As we are dealing with images, we will use a convolutional neural network (CNN)."]},{"cell_type":"markdown","metadata":{},"source":["Here below, we proposed a CNN composed of several layers:\n","- Convolutional: allow to detect pattern inside an image\n","- Batchnormalization: avoid overfitting by normalizing\n","- Max pooling: select the maximum of the region covered by the filter\n","- Dropout: nulifies the contribution of some neurons\n","- Dense: at the end of the NN when the layer is flattened to finally have N outputs corresponding to the number of classes"]},{"cell_type":"markdown","metadata":{},"source":["It is important to notice that the activation function at the end of the network is a sigmoid function and not a softmax function as we are dealing with a multi-label classification problem (several classes are possible for one instance)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def CNN():\n","  model = Sequential()\n","  input_shape = (64,64,3)\n","  channel_dim = -1\n","  if bck.image_data_format() == \"channels_first\":\n","    inputShape = (depth, height, width)\n","    channel_dim = 1\n","  model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=input_shape))\n","  model.add(Activation(\"relu\"))\n","  model.add(BatchNormalization(axis=channel_dim))\n","  model.add(MaxPooling2D(pool_size=(3, 3)))\n","  model.add(Dropout(0.25))\n","\n","  model.add(Conv2D(64, (3, 3), padding=\"same\"))\n","  model.add(Activation(\"relu\"))\n","  model.add(BatchNormalization(axis=channel_dim))\n","  model.add(Conv2D(64, (3, 3), padding=\"same\"))\n","  model.add(Activation(\"relu\"))\n","  model.add(BatchNormalization(axis=channel_dim))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(0.25))\n","\n","  model.add(Conv2D(128, (3, 3), padding=\"same\"))\n","  model.add(Activation(\"relu\"))\n","  model.add(BatchNormalization(axis=channel_dim))\n","  model.add(Conv2D(128, (3, 3), padding=\"same\"))\n","  model.add(Activation(\"relu\"))\n","  model.add(BatchNormalization(axis=channel_dim))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(0.25))\n","\n","  model.add(Flatten())\n","  model.add(Dense(1024))\n","  model.add(Activation(\"relu\"))\n","  model.add(BatchNormalization())\n","  model.add(Dropout(0.5))\n","  model.add(Dense(20)) \n","  model.add(Activation('sigmoid'))\n","  \n","  return model"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"markdown","metadata":{},"source":["Now we put everything together and we can start the training of our classification problem."]},{"cell_type":"markdown","metadata":{},"source":["Parameters initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = CNN()\n","optimizer = 'adam'\n","batch_size = 32\n","epochs = 10\n","filepath = 'save_CNN'"]},{"cell_type":"markdown","metadata":{},"source":["Conversion Int64 to Float32"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_train = np.float32(y_train)\n","y_test = np.float32(y_test)"]},{"cell_type":"markdown","metadata":{},"source":["### Data augmentation"]},{"cell_type":"markdown","metadata":{},"source":["To improve the learning of the model we will use data augmentation, i.e. we add data to our dataset to have more samples to train on. To do so, we apply several transformations to our initial images such as rotation, translation etc."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["save_best = ModelCheckpoint(filepath, monitor='val_get_f1', verbose=0, save_best_only=True, mode='max', period=1)\n","\n","model.compile(loss=loss_binary_weights(class_weights),optimizer=optimizer,metrics=[f_beta_1])\n","\n","# data augmentation\n","datagen = ImageDataGenerator(rotation_range=90, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2,zoom_range=0.2, fill_mode=\"nearest\", horizontal_flip=True)\n","\n","datagen.fit(X_train)\n","\n","# training\n","history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size), validation_data=(X_test, y_test), steps_per_epoch = len(X_train) // batch_size, epochs=epochs, verbose=1, workers=4, callbacks=[save_best], shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Results"]},{"cell_type":"markdown","metadata":{},"source":["Plot training and validation losses"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.ylim(top=5)\n","plt.plot(np.arange(0, epochs), history.history[\"loss\"], label=\"training loss\")\n","plt.plot(np.arange(0, epochs), history.history[\"val_loss\"], label=\"validation loss\")\n","plt.title(\"Training result\")\n","plt.xlabel(\"Epoch\")\n","plt.legend(loc=\"upper right\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Plot training and validation accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.ylim(top=1)\n","plt.plot(np.arange(0, epochs), history.history[\"f_beta_1\"], label=\"training F1\")\n","plt.plot(np.arange(0, epochs), history.history[\"val_f_beta_1\"], label=\"validation F1\")\n","plt.title(\"Training result\")\n","plt.xlabel(\"Epoch\")\n","plt.legend(loc=\"upper right\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Based on https://github.com/kenanEkici/multilabel-class-pascalvoc\n","\n","threshold = 0.4\n","\n","y_pred = model.predict(X_test)\n","\n","y_pred[y_pred>=threshold] = 1\n","y_pred[y_pred<threshold] = 0\n","\n","print('Precision: ', precision_score(y_test, y_pred, average='samples', zero_division=0)) \n","print('Recall: ', recall_score(y_test, y_pred, average='samples')) \n","print('F beta 1 score: ', f1_score(y_test, y_pred, average='samples'), '\\n') \n","\n","print('Per class F beta 1 score: ')\n","f1_scores = f1_score(y_test, y_pred, average=None)\n","for i in range(len(f1_scores)):\n","  print(list(sorted(labels))[i], round(f1_scores[i],2))"]},{"cell_type":"markdown","metadata":{},"source":["To chose the threshold, let us plot the precision-recall curve (for a balanced dataset, we use the ROC curve, for an unbalanced, the precision-recall curve). The maximum corresponds to the threshold giving the best accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Based on https://github.com/kenanEkici/multilabel-class-pascalvoc\n","\n","fig, ax = plt.subplots(figsize= (5,5))\n","ax.set_title(\"Precision recall\")\n","ax.set_ylabel(\"Precision\")\n","ax.set_xlabel(\"Recall\")\n","ax.set_xlim(xmin=0, xmax=1)\n","ax.set_ylim(ymin=0, ymax=1)\n","\n","x = []\n","y = []\n","for threshold in np.arange(0, 1.1, 0.1):\n","  y_pred = model.predict(X_test)\n","  y_pred[y_pred>=threshold] = 1\n","  y_pred[y_pred<threshold] = 0\n","  p = precision_score(y_test, y_pred, average='samples', zero_division=0)\n","  r = recall_score(y_test, y_pred, average='samples')\n","  ax.annotate(str(round(threshold,2)), (r, p))\n","  x.append(r)\n","  y.append(p)\n","  \n","ax.plot(x, y)\n","ax.lines[-1].set_label(\"CNN\")\n","\n","plt.legend(loc=\"upper right\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We notice that our model does not learn very well. To improve it, we will use what we called transfer learning."]},{"cell_type":"markdown","metadata":{},"source":["## Transfer learning"]},{"cell_type":"markdown","metadata":{},"source":["This consists of using an already trained NN. First we transfer the learning of this model on our own and then we apply fine tuning by updating the parameters of the model by using our training dataset."]},{"cell_type":"markdown","metadata":{},"source":["We will use MobilNet which provides an already trained NN. It is base on the ImageNet dataset which consists of million of images with thousand of classes."]},{"cell_type":"markdown","metadata":{},"source":["We set the input size to 224 as the pre-trained model works with 224x224 images"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["im_size = 224"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["processed_train_data_images = np.zeros((train_size, im_size,im_size,3))\n","processed_test_data_images = np.zeros((test_size, im_size,im_size,3))\n","\n","for i in range(train_size):\n","  processed_train_data_images[i] = preprocess(train_df.loc[i]['img'], im_size)\n","\n","for i in range(train_size):\n","  processed_test_data_images[i] = preprocess(test_df.loc[i]['img'], im_size)\n","\n","processed_train_data_labels = np.array(train_df.drop(['img', 'seg'], axis=1))\n","\n","X_train, X_test, y_train, y_test = train_test_split(processed_train_data_images, processed_train_data_labels, test_size=0.33)"]},{"cell_type":"markdown","metadata":{},"source":["Let use create our pre-trained model"]},{"cell_type":"markdown","metadata":{},"source":["Parameters initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["optimizer = 'adam'\n","batch_size = 32\n","epochs = 40\n","filepath = 'save_CNN'"]},{"cell_type":"markdown","metadata":{},"source":["Here we will use the CNN of the MobileNet model. To speed up the convergence we will replace the hidden layer at the end with a global average pooling (GAP) layer. The GAP layer allows to go from a 3D map directly to a flattened layer of 20 outputs (because we have 20 classes). So we go from a height x width x depth matrix to a 1 x 1 x depth. For each heigh x weight map it takes the mean to have a the end only a dimension of 1. This decreases the numbers of parameters to learn and thus leads to a faster convergence."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.applications import MobileNetV2\n","\n","base_model = MobileNetV2(input_shape=(224,224,3), include_top=False, weights='imagenet')\n","base_model.trainable = False\n","model = Sequential([base_model,GlobalAveragePooling2D(),Dense(20, activation='sigmoid')])\n","\n","model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[f_beta_1])"]},{"cell_type":"markdown","metadata":{},"source":["Here again, we will use data augmentation to improve the learning of our model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["datagen = ImageDataGenerator(rotation_range=40, width_shift_range=0.2 ,height_shift_range=0.2, shear_range=0.2,zoom_range=0.2, fill_mode=\"nearest\", horizontal_flip=True)\n","datagen.fit(X_train)\n","\n","save_best = ModelCheckpoint(filepath, monitor='val_f_beta_1', verbose=0, save_best_only=True, mode='max', period=1)\n","\n","# training the binary classifier\n","history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),validation_data=(X_test, y_test),epochs=epochs, verbose=1, workers=4, shuffle=True, callbacks=[save_best])"]},{"cell_type":"markdown","metadata":{},"source":["### Results"]},{"cell_type":"markdown","metadata":{},"source":["Plot training and validation losses"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.ylim(top=1)\n","plt.plot(np.arange(0, epochs), history.history[\"loss\"], label=\"training loss\")\n","plt.plot(np.arange(0, epochs), history.history[\"val_loss\"], label=\"validation loss\")\n","plt.title(\"Training result\")\n","plt.xlabel(\"Epoch\")\n","plt.legend(loc=\"upper right\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Plot training and validation accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.ylim(top=1)\n","plt.plot(np.arange(0, epochs), history.history[\"f_beta_1\"], label=\"training F1\")\n","plt.plot(np.arange(0, epochs), history.history[\"val_f_beta_1\"], label=\"validation F1\")\n","plt.title(\"Training result\")\n","plt.xlabel(\"Epoch\")\n","plt.legend(loc=\"upper right\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Adversarial attacks"]},{"cell_type":"markdown","metadata":{},"source":["In this section, the goal is to full our model by misclassifying some images. To simplify the problem let us just consider a binary classification. We choose two sets of similar sices, the ones of car's and sofa's. A car and sofa also have kind of the same shape too not make this task too easy for us."]},{"cell_type":"code","execution_count":164,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T18:51:40.681118Z","iopub.status.busy":"2022-05-20T18:51:40.680398Z","iopub.status.idle":"2022-05-20T18:51:40.686319Z","shell.execute_reply":"2022-05-20T18:51:40.685579Z","shell.execute_reply.started":"2022-05-20T18:51:40.681082Z"},"trusted":true},"outputs":[],"source":["## Binary Classifier\n","class_1 = 'car'\n","class_2 = 'sofa'\n","\n","train_bin = train_df[['img', class_1, class_2]].copy()"]},{"cell_type":"code","execution_count":165,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T18:51:41.039273Z","iopub.status.busy":"2022-05-20T18:51:41.038535Z","iopub.status.idle":"2022-05-20T18:51:41.044071Z","shell.execute_reply":"2022-05-20T18:51:41.043217Z","shell.execute_reply.started":"2022-05-20T18:51:41.039236Z"},"trusted":true},"outputs":[],"source":["def label_to_string(label_):\n","    if label_ == 0:\n","        return class_1\n","    if label_ == 1:\n","         return class_2"]},{"cell_type":"code","execution_count":214,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:19.879897Z","iopub.status.busy":"2022-05-20T19:02:19.879641Z","iopub.status.idle":"2022-05-20T19:02:19.884948Z","shell.execute_reply":"2022-05-20T19:02:19.884279Z","shell.execute_reply.started":"2022-05-20T19:02:19.879871Z"},"trusted":true},"outputs":[],"source":["im_size = 64\n","\n","def preprocess(img, im_size):\n","  #resize\n","  img = cv2.resize(img, dsize=(im_size, im_size), interpolation=cv2.INTER_LINEAR)\n","  \n","  #normalize\n","  img = img/255.0\n","  \n","  return img.astype('float32')"]},{"cell_type":"code","execution_count":215,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:20.930126Z","iopub.status.busy":"2022-05-20T19:02:20.929382Z","iopub.status.idle":"2022-05-20T19:02:21.162117Z","shell.execute_reply":"2022-05-20T19:02:21.161337Z","shell.execute_reply.started":"2022-05-20T19:02:20.930082Z"},"trusted":true},"outputs":[],"source":["train_data_images = []\n","train_data_labels = []\n","\n","for i in range(train_size):\n","  bool_ = (train_bin[class_1].loc[i] == 1 and train_bin[class_2].loc[i] == 0) or (train_bin.loc[i][class_1] == 0 and train_bin.loc[i][class_2] == 1)\n","  if bool_:\n","    train_data_images.append(preprocess( train_bin['img'].iloc[i],im_size))\n","\n","    if train_bin[class_1].iloc[i]==1:\n","      train_data_labels.append(0)\n","    else:\n","      train_data_labels.append(1)\n","\n","train_data_images = np.array(train_data_images)\n","train_data_labels = np.array(train_data_labels)"]},{"cell_type":"code","execution_count":216,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:21.285470Z","iopub.status.busy":"2022-05-20T19:02:21.285124Z","iopub.status.idle":"2022-05-20T19:02:21.293184Z","shell.execute_reply":"2022-05-20T19:02:21.292406Z","shell.execute_reply.started":"2022-05-20T19:02:21.285438Z"},"trusted":true},"outputs":[],"source":["X_train, X_val, y_train, y_val = train_test_split(train_data_images, train_data_labels, test_size=0.15)"]},{"cell_type":"code","execution_count":217,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:22.471601Z","iopub.status.busy":"2022-05-20T19:02:22.471335Z","iopub.status.idle":"2022-05-20T19:02:22.662227Z","shell.execute_reply":"2022-05-20T19:02:22.660676Z","shell.execute_reply.started":"2022-05-20T19:02:22.471571Z"},"trusted":true},"outputs":[],"source":["#car\n","\n","car_label = y_train[y_train==0][0]\n","car = X_train[y_train==0][0]\n","print('label', car_label, '->', label_to_string(0))\n","plt.imshow(car)\n","plt.show()"]},{"cell_type":"code","execution_count":218,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:23.836726Z","iopub.status.busy":"2022-05-20T19:02:23.836369Z","iopub.status.idle":"2022-05-20T19:02:24.019188Z","shell.execute_reply":"2022-05-20T19:02:24.018311Z","shell.execute_reply.started":"2022-05-20T19:02:23.836685Z"},"trusted":true},"outputs":[],"source":["#sofa\n","\n","sofa_label = y_train[y_train==1][0]\n","sofa = X_train[y_train==1][0]\n","print('label', sofa_label, '->', label_to_string(1))\n","plt.imshow(sofa)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Now let us run our model. To do so we will again use transfer learning. As we our doing a binary classification, we just have one output at the end of the NN."]},{"cell_type":"code","execution_count":219,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:25.763286Z","iopub.status.busy":"2022-05-20T19:02:25.763024Z","iopub.status.idle":"2022-05-20T19:02:25.767368Z","shell.execute_reply":"2022-05-20T19:02:25.766448Z","shell.execute_reply.started":"2022-05-20T19:02:25.763259Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.applications import MobileNetV2"]},{"cell_type":"code","execution_count":220,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:26.743434Z","iopub.status.busy":"2022-05-20T19:02:26.743189Z","iopub.status.idle":"2022-05-20T19:02:27.976284Z","shell.execute_reply":"2022-05-20T19:02:27.975566Z","shell.execute_reply.started":"2022-05-20T19:02:26.743406Z"},"trusted":true},"outputs":[],"source":["base_model = MobileNetV2(input_shape=(im_size,im_size,3), include_top=False, weights='imagenet')\n","base_model.trainable = False\n","model = Sequential([\n","  base_model,\n","  GlobalAveragePooling2D(),\n","  Dense(1, activation='sigmoid')\n","])\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":221,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:37.883407Z","iopub.status.busy":"2022-05-20T19:02:37.882724Z","iopub.status.idle":"2022-05-20T19:02:46.008252Z","shell.execute_reply":"2022-05-20T19:02:46.007306Z","shell.execute_reply.started":"2022-05-20T19:02:37.883370Z"},"trusted":true},"outputs":[],"source":["##with data augmentation\n","# datagen = ImageDataGenerator(rotation_range=40, width_shift_range=0.2,height_shift_range=0.2, shear_range=0.2,zoom_range=0.2,fill_mode=\"nearest\", horizontal_flip=True)\n","\n","# datagen.fit(X_train)\n","\n","# history = model.fit(datagen.flow(X_train, y_train, batch_size=4), validation_data=(X_val, y_val), epochs=10, verbose=1, workers=4, shuffle=True)\n","\n","##without data augmentation\n","history = model.fit(X_train, y_train, batch_size=4, validation_data=(X_val, y_val), epochs=10, verbose=1)"]},{"cell_type":"code","execution_count":222,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:46.014174Z","iopub.status.busy":"2022-05-20T19:02:46.013919Z","iopub.status.idle":"2022-05-20T19:02:46.513207Z","shell.execute_reply":"2022-05-20T19:02:46.512457Z","shell.execute_reply.started":"2022-05-20T19:02:46.014140Z"},"trusted":true},"outputs":[],"source":["[score,acc] = model.evaluate(X_train, y_train)\n","[score,acc] = model.evaluate(X_val, y_val)"]},{"cell_type":"markdown","metadata":{},"source":["Because of our data augmentation the final accuracy does not correspond to the final output accuracy of the training.\n","The accuracy is very good."]},{"cell_type":"markdown","metadata":{},"source":["### Aversarial model"]},{"cell_type":"markdown","metadata":{},"source":["#### Deceptive labels"]},{"cell_type":"code","execution_count":223,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:52.401611Z","iopub.status.busy":"2022-05-20T19:02:52.400894Z","iopub.status.idle":"2022-05-20T19:02:52.409210Z","shell.execute_reply":"2022-05-20T19:02:52.408421Z","shell.execute_reply.started":"2022-05-20T19:02:52.401563Z"},"trusted":true},"outputs":[],"source":["# make false labels\n","y_train_dec = y_train*-1+1\n","y_val_dec = y_val*-1+1\n","\n","print(\"Check\")\n","print(y_train[:5])\n","print(y_train_dec[:5])"]},{"cell_type":"markdown","metadata":{},"source":["#### Encoder-decoder model"]},{"cell_type":"code","execution_count":208,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:00:47.607615Z","iopub.status.busy":"2022-05-20T19:00:47.607339Z","iopub.status.idle":"2022-05-20T19:00:47.612038Z","shell.execute_reply":"2022-05-20T19:00:47.610899Z","shell.execute_reply.started":"2022-05-20T19:00:47.607578Z"},"trusted":true},"outputs":[],"source":["from keras.layers import Input, Lambda, UpSampling2D"]},{"cell_type":"code","execution_count":224,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:54.807533Z","iopub.status.busy":"2022-05-20T19:02:54.807236Z","iopub.status.idle":"2022-05-20T19:02:54.959383Z","shell.execute_reply":"2022-05-20T19:02:54.958650Z","shell.execute_reply.started":"2022-05-20T19:02:54.807486Z"},"trusted":true},"outputs":[],"source":["#adversary network\n","\n","input_shape = (im_size,im_size,3)\n","\n","input_img = Input(shape=input_shape)\n","x = Conv2D(64, (3, 3), padding='same')(input_img)\n","x = BatchNormalization()(x)\n","x = Activation('relu')(x)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","x = Conv2D(32, (3, 3), padding='same')(x)\n","x = BatchNormalization()(x)\n","x = Activation('relu')(x)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","x = Conv2D(16, (3, 3), padding='same')(x)\n","x = BatchNormalization()(x)\n","x = Activation('relu')(x)\n","encoded = MaxPooling2D((2, 2), padding='same')(x)\n","\n","x = Conv2D(16, (3, 3), padding='same')(encoded)\n","x = BatchNormalization()(x)\n","x = Activation('relu')(x)\n","x = UpSampling2D((2, 2))(x)\n","x = Conv2D(32, (3, 3), padding='same')(x)\n","x = BatchNormalization()(x)\n","x = Activation('relu')(x)\n","x = UpSampling2D((2, 2))(x)\n","x = Conv2D(64, (3, 3), padding='same')(x)\n","x = BatchNormalization()(x)\n","x = Activation('relu')(x)\n","x = UpSampling2D((2, 2))(x)\n","x = Conv2D(3, (3, 3), padding='same')(x)\n","x = BatchNormalization()(x)\n","decoded = Activation('sigmoid')(x)"]},{"cell_type":"markdown","metadata":{},"source":["Add perturbation to our input image"]},{"cell_type":"code","execution_count":225,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:57.814403Z","iopub.status.busy":"2022-05-20T19:02:57.814157Z","iopub.status.idle":"2022-05-20T19:02:57.818232Z","shell.execute_reply":"2022-05-20T19:02:57.817081Z","shell.execute_reply.started":"2022-05-20T19:02:57.814373Z"},"trusted":true},"outputs":[],"source":["eps_ = 100."]},{"cell_type":"code","execution_count":226,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:02:58.946993Z","iopub.status.busy":"2022-05-20T19:02:58.946401Z","iopub.status.idle":"2022-05-20T19:02:58.968770Z","shell.execute_reply":"2022-05-20T19:02:58.967990Z","shell.execute_reply.started":"2022-05-20T19:02:58.946956Z"},"trusted":true},"outputs":[],"source":["def perturb(args):\n","    decoded,input_img,eps = args\n","    return decoded*bck.minimum(bck.pow(bck.sqrt(bck.sum(bck.square(decoded))),-1)*eps,1) \n","\n","# Add perturbation\n","def perturb_img(args):\n","    perturbation,input_img = args\n","    return bck.clip(perturbation+input_img,0,1)\n","\n","perturbation = Lambda(perturb, output_shape=input_shape, name='perturb')([decoded,input_img, eps_])\n","perturbed_img = Lambda(perturb_img, output_shape=input_shape, name='perturb_img')([perturbation,input_img])"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-05-20T18:34:51.170959Z","iopub.status.busy":"2022-05-20T18:34:51.170034Z","iopub.status.idle":"2022-05-20T18:34:51.489357Z","shell.execute_reply":"2022-05-20T18:34:51.488633Z","shell.execute_reply.started":"2022-05-20T18:34:51.170922Z"}},"source":["##### Make sure that our clean network is not trained, the weights are frozen."]},{"cell_type":"code","execution_count":227,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:03:02.293656Z","iopub.status.busy":"2022-05-20T19:03:02.293083Z","iopub.status.idle":"2022-05-20T19:03:02.302231Z","shell.execute_reply":"2022-05-20T19:03:02.301576Z","shell.execute_reply.started":"2022-05-20T19:03:02.293621Z"},"trusted":true},"outputs":[],"source":["model.trainable = False"]},{"cell_type":"code","execution_count":228,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:03:03.623122Z","iopub.status.busy":"2022-05-20T19:03:03.622432Z","iopub.status.idle":"2022-05-20T19:03:03.927201Z","shell.execute_reply":"2022-05-20T19:03:03.926478Z","shell.execute_reply.started":"2022-05-20T19:03:03.623088Z"},"trusted":true},"outputs":[],"source":["outputs = model(perturbed_img)"]},{"cell_type":"markdown","metadata":{},"source":["Train the adversary model."]},{"cell_type":"markdown","metadata":{},"source":["In the verbose of the training we can very nicely see that the accuracy of the validation set lowers when being compared to it's correct labels."]},{"cell_type":"code","execution_count":229,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:03:06.358126Z","iopub.status.busy":"2022-05-20T19:03:06.357434Z","iopub.status.idle":"2022-05-20T19:03:21.093288Z","shell.execute_reply":"2022-05-20T19:03:21.092557Z","shell.execute_reply.started":"2022-05-20T19:03:06.358093Z"},"trusted":true},"outputs":[],"source":["batch_size = 4\n","epochs = 20\n","\n","adv_CNN = Model(input_img, outputs, name='adv')\n","adv_CNN.compile(optimizer='adam',loss='binary_crossentropy', metrics = ['accuracy'])\n","\n","history = adv_CNN.fit(X_train, y_train_dec,validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs,verbose=1)"]},{"cell_type":"markdown","metadata":{},"source":["Check of the accuracy of the network is lowered"]},{"cell_type":"code","execution_count":230,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:04:35.800979Z","iopub.status.busy":"2022-05-20T19:04:35.800359Z","iopub.status.idle":"2022-05-20T19:04:36.275920Z","shell.execute_reply":"2022-05-20T19:04:36.275145Z","shell.execute_reply.started":"2022-05-20T19:04:35.800931Z"},"trusted":true},"outputs":[],"source":["[score,acc] = adv_CNN.evaluate(X_train, y_train_dec)\n","[score,acc] = adv_CNN.evaluate(X_train, y_train)"]},{"cell_type":"code","execution_count":231,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:04:39.216566Z","iopub.status.busy":"2022-05-20T19:04:39.216300Z","iopub.status.idle":"2022-05-20T19:04:39.385166Z","shell.execute_reply":"2022-05-20T19:04:39.384492Z","shell.execute_reply.started":"2022-05-20T19:04:39.216536Z"},"trusted":true},"outputs":[],"source":["[score,acc] = adv_CNN.evaluate(X_val, y_val_dec)\n","[score,acc] = adv_CNN.evaluate(X_val, y_val)"]},{"cell_type":"code","execution_count":232,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:04:41.765871Z","iopub.status.busy":"2022-05-20T19:04:41.765136Z","iopub.status.idle":"2022-05-20T19:04:41.878397Z","shell.execute_reply":"2022-05-20T19:04:41.877676Z","shell.execute_reply.started":"2022-05-20T19:04:41.765823Z"},"trusted":true},"outputs":[],"source":["# check if the original network still works\n","[score,acc] = model.evaluate(X_train, y_train)"]},{"cell_type":"code","execution_count":233,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:04:43.853704Z","iopub.status.busy":"2022-05-20T19:04:43.853250Z","iopub.status.idle":"2022-05-20T19:04:45.034384Z","shell.execute_reply":"2022-05-20T19:04:45.033621Z","shell.execute_reply.started":"2022-05-20T19:04:43.853666Z"},"trusted":true},"outputs":[],"source":["adv_model = adv_CNN\n","inter_layer_perturb_img = Model(inputs=adv_model.input,outputs=adv_model.get_layer('perturb_img').output)\n","inter_layer_perturb = Model(inputs=adv_model.input,outputs=adv_model.get_layer('perturb').output)\n","generated_img = inter_layer_perturb_img.predict(X_train)\n","perturbation = inter_layer_perturb.predict(X_train)\n","output = adv_model.predict(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_train = np.round(output).reshape(len(X_train))\n","succesfull_fakes = X_train[abs(pred_train - y_train).astype(bool)]\n","\n","acc_ = sum(abs(pred_train - y_train).astype(bool))/len(y_train)\n","print('Percentage of wrongfully classified images:', acc_)"]},{"cell_type":"code","execution_count":234,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:04:47.039430Z","iopub.status.busy":"2022-05-20T19:04:47.039184Z","iopub.status.idle":"2022-05-20T19:04:49.288033Z","shell.execute_reply":"2022-05-20T19:04:49.287324Z","shell.execute_reply.started":"2022-05-20T19:04:47.039403Z"},"trusted":true},"outputs":[],"source":["#plot some results which where correctly classified at first and now wrong\n","img_i = 0\n","for img_ in succesfull_fakes:\n","    if img_i<6:\n","        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14,10))\n","        ax1.imshow(X_train[img_i])\n","        ax1.set_title('OG')\n","        ax2.imshow(generated_img[img_i])\n","        ax2.set_title('Perturberd Image')\n","        ax3.imshow(perturbation[img_i])\n","        ax3.set_title('Perturbation')\n","        plt.show()\n","        print('Real Label:', y_train[img_i], '->', label_to_string(y_train[img_i]))\n","        print('Adversary Label:', y_train_dec[img_i], '->', label_to_string(y_train_dec[img_i]))\n","    img_i+=1\n","    "]},{"cell_type":"markdown","metadata":{},"source":["### Do the same for a lower eps_ value."]},{"cell_type":"code","execution_count":235,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:06:47.564639Z","iopub.status.busy":"2022-05-20T19:06:47.564253Z","iopub.status.idle":"2022-05-20T19:06:47.569409Z","shell.execute_reply":"2022-05-20T19:06:47.568648Z","shell.execute_reply.started":"2022-05-20T19:06:47.564594Z"},"trusted":true},"outputs":[],"source":["eps_ = 10."]},{"cell_type":"code","execution_count":236,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:06:48.294096Z","iopub.status.busy":"2022-05-20T19:06:48.293468Z","iopub.status.idle":"2022-05-20T19:06:48.317295Z","shell.execute_reply":"2022-05-20T19:06:48.316588Z","shell.execute_reply.started":"2022-05-20T19:06:48.294054Z"},"trusted":true},"outputs":[],"source":["def perturb(args):\n","    decoded,input_img,eps = args\n","    return decoded*bck.minimum(bck.pow(bck.sqrt(bck.sum(bck.square(decoded))),-1)*eps,1) \n","\n","# Add perturbation\n","def perturb_img(args):\n","    perturbation,input_img = args\n","    return bck.clip(perturbation+input_img,0,1)\n","\n","perturbation = Lambda(perturb, output_shape=input_shape, name='perturb')([decoded,input_img, eps_])\n","perturbed_img = Lambda(perturb_img, output_shape=input_shape, name='perturb_img')([perturbation,input_img])"]},{"cell_type":"code","execution_count":237,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:06:50.048884Z","iopub.status.busy":"2022-05-20T19:06:50.048231Z","iopub.status.idle":"2022-05-20T19:07:04.894982Z","shell.execute_reply":"2022-05-20T19:07:04.894059Z","shell.execute_reply.started":"2022-05-20T19:06:50.048844Z"},"trusted":true},"outputs":[],"source":["model.trainable = False\n","outputs = model(perturbed_img)\n","\n","batch_size = 4\n","epochs = 20\n","\n","adv_CNN = Model(input_img, outputs, name='adv')\n","adv_CNN.compile(optimizer='adam',loss='binary_crossentropy', metrics = ['accuracy'])\n","\n","history = adv_CNN.fit(X_train, y_train_dec,validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs,verbose=1)"]},{"cell_type":"code","execution_count":238,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:07:12.081377Z","iopub.status.busy":"2022-05-20T19:07:12.081118Z","iopub.status.idle":"2022-05-20T19:07:14.607616Z","shell.execute_reply":"2022-05-20T19:07:14.606862Z","shell.execute_reply.started":"2022-05-20T19:07:12.081349Z"},"trusted":true},"outputs":[],"source":["adv_model = adv_CNN\n","inter_layer_perturb_img = Model(inputs=adv_model.input,outputs=adv_model.get_layer('perturb_img').output)\n","inter_layer_perturb = Model(inputs=adv_model.input,outputs=adv_model.get_layer('perturb').output)\n","generated_img = inter_layer_perturb_img.predict(X_train)\n","perturbation = inter_layer_perturb.predict(X_train)\n","output = adv_model.predict(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_train = np.round(output).reshape(len(X_train))\n","succesfull_fakes = X_train[abs(pred_train - y_train).astype(bool)]\n","\n","acc_ = sum(abs(pred_train - y_train).astype(bool))/len(y_train)\n","print('Percentage of wrongfully classified images:', acc_)"]},{"cell_type":"code","execution_count":240,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:10:04.246372Z","iopub.status.busy":"2022-05-20T19:10:04.245554Z","iopub.status.idle":"2022-05-20T19:10:04.618957Z","shell.execute_reply":"2022-05-20T19:10:04.618222Z","shell.execute_reply.started":"2022-05-20T19:10:04.246330Z"},"trusted":true},"outputs":[],"source":["#plot some results which where correctly classified at first and now wrong\n","img_i = 0\n","for img_ in succesfull_fakes:\n","    if img_i<6:\n","        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14,10))\n","        ax1.imshow(X_train[img_i])\n","        ax1.set_title('OG')\n","        ax2.imshow(generated_img[img_i])\n","        ax2.set_title('Perturberd Image')\n","        ax3.imshow(perturbation[img_i])\n","        ax3.set_title('Perturbation')\n","        plt.show()\n","        print('Real Label:', y_train[img_i], '->', label_to_string(y_train[img_i]))\n","        print('Adversary Label:', y_train_dec[img_i], '->', label_to_string(y_train_dec[img_i]))\n","    img_i+=1\n","    "]},{"cell_type":"markdown","metadata":{},"source":["When using a lower perturbation not many pictures will be wrongfully classified as expected."]},{"cell_type":"markdown","metadata":{},"source":["### Do the same for a higher eps_ value."]},{"cell_type":"code","execution_count":241,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:10:11.783453Z","iopub.status.busy":"2022-05-20T19:10:11.783192Z","iopub.status.idle":"2022-05-20T19:10:11.787683Z","shell.execute_reply":"2022-05-20T19:10:11.786655Z","shell.execute_reply.started":"2022-05-20T19:10:11.783424Z"},"trusted":true},"outputs":[],"source":["eps_ = 600."]},{"cell_type":"code","execution_count":242,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:10:12.121021Z","iopub.status.busy":"2022-05-20T19:10:12.120459Z","iopub.status.idle":"2022-05-20T19:10:12.143896Z","shell.execute_reply":"2022-05-20T19:10:12.143222Z","shell.execute_reply.started":"2022-05-20T19:10:12.120983Z"},"trusted":true},"outputs":[],"source":["def perturb(args):\n","    decoded,input_img,eps = args\n","    return decoded*bck.minimum(bck.pow(bck.sqrt(bck.sum(bck.square(decoded))),-1)*eps,1) \n","\n","# Add perturbation\n","def perturb_img(args):\n","    perturbation,input_img = args\n","    return bck.clip(perturbation+input_img,0,1)\n","\n","perturbation = Lambda(perturb, output_shape=input_shape, name='perturb')([decoded,input_img, eps_])\n","perturbed_img = Lambda(perturb_img, output_shape=input_shape, name='perturb_img')([perturbation,input_img])"]},{"cell_type":"code","execution_count":243,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:10:12.697464Z","iopub.status.busy":"2022-05-20T19:10:12.697216Z","iopub.status.idle":"2022-05-20T19:10:37.035006Z","shell.execute_reply":"2022-05-20T19:10:37.034251Z","shell.execute_reply.started":"2022-05-20T19:10:12.697434Z"},"trusted":true},"outputs":[],"source":["model.trainable = False\n","outputs = model(perturbed_img)\n","\n","batch_size = 4\n","epochs = 20\n","\n","adv_CNN = Model(input_img, outputs, name='adv')\n","adv_CNN.compile(optimizer='adam',loss='binary_crossentropy', metrics = ['accuracy'])\n","\n","history = adv_CNN.fit(X_train, y_train_dec,validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs,verbose=1)"]},{"cell_type":"code","execution_count":244,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:10:52.332203Z","iopub.status.busy":"2022-05-20T19:10:52.331488Z","iopub.status.idle":"2022-05-20T19:10:53.562175Z","shell.execute_reply":"2022-05-20T19:10:53.561403Z","shell.execute_reply.started":"2022-05-20T19:10:52.332167Z"},"trusted":true},"outputs":[],"source":["adv_model = adv_CNN\n","inter_layer_perturb_img = Model(inputs=adv_model.input,outputs=adv_model.get_layer('perturb_img').output)\n","inter_layer_perturb = Model(inputs=adv_model.input,outputs=adv_model.get_layer('perturb').output)\n","generated_img = inter_layer_perturb_img.predict(X_train)\n","perturbation = inter_layer_perturb.predict(X_train)\n","output = adv_model.predict(X_train)"]},{"cell_type":"code","execution_count":249,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:13:23.296329Z","iopub.status.busy":"2022-05-20T19:13:23.296052Z","iopub.status.idle":"2022-05-20T19:13:23.303918Z","shell.execute_reply":"2022-05-20T19:13:23.303218Z","shell.execute_reply.started":"2022-05-20T19:13:23.296297Z"},"trusted":true},"outputs":[],"source":["pred_train = np.round(output).reshape(len(X_train))\n","succesfull_fakes = X_train[abs(pred_train - y_train).astype(bool)]\n","\n","acc_ = sum(abs(pred_train - y_train).astype(bool))/len(y_train)\n","print('Percentage of wrongfully classified images:', acc_)"]},{"cell_type":"code","execution_count":245,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T19:10:54.856850Z","iopub.status.busy":"2022-05-20T19:10:54.856130Z","iopub.status.idle":"2022-05-20T19:10:57.089450Z","shell.execute_reply":"2022-05-20T19:10:57.088695Z","shell.execute_reply.started":"2022-05-20T19:10:54.856810Z"},"trusted":true},"outputs":[],"source":["#plot some results which where correctly classified at first and now wrong\n","\n","img_i = 0\n","for img_ in succesfull_fakes:\n","    if img_i<6:\n","        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14,10))\n","        ax1.imshow(X_train[img_i])\n","        ax1.set_title('OG')\n","        ax2.imshow(generated_img[img_i])\n","        ax2.set_title('Perturberd Image')\n","        ax3.imshow(perturbation[img_i])\n","        ax3.set_title('Perturbation')\n","        plt.show()\n","        print('Real Label:', y_train[img_i], '->', label_to_string(y_train[img_i]))\n","        print('Adversary Label:', y_train_dec[img_i], '->', label_to_string(y_train_dec[img_i]))\n","    img_i+=1"]},{"cell_type":"markdown","metadata":{},"source":["# Conclusion"]},{"cell_type":"markdown","metadata":{},"source":["Adversary: using a higher resolution usually means that a higher perturbation value is needed.\n","When using a high perturbation value the image is such distorted that it is not even the same image. It is logical that image would be easier to distort."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
