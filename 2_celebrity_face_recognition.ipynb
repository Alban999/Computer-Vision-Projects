{"cells":[{"cell_type":"markdown","metadata":{"id":"qoZihqFtbTTW"},"source":["Downoald data set from competition"]},{"cell_type":"markdown","metadata":{"id":"NhzeS1VAa3U8","papermill":{"duration":0.043808,"end_time":"2022-04-09T17:04:07.581584","exception":false,"start_time":"2022-04-09T17:04:07.537776","status":"completed"},"tags":[]},"source":["<div style=\"width:100%; height:140px\">\n","    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=down>\n","</div>\n","\n","\n","KUL H02A5a Computer Vision: Group Assignment 1\n","---------------------------------------------------------------\n","Student numbers: <span style=\"color:red\">r0893875, r0877856, r0745139, r0880442</span>.\n","\n","The goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n","\n","In this assignment you are a group of computer vision experts that have been invited to ECCV 2021 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n","\n","---------------------------------------------------------------\n","This notebook is structured as follows:\n","0. Data loading & Preprocessing\n","1. Feature Representations\n","2. Evaluation Metrics \n","3. Classifiers\n","4. Experiments\n","5. Publishing best results\n","6. Discussion\n","\n","Make sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n","\n","Fill in your student numbers above and get to it! Good luck! "]},{"cell_type":"markdown","metadata":{"id":"YC0m4Y0roAzv"},"source":["# 0. Data loading & Preprocessing\n","\n","We load and preprocess the VGG faces dataset. The training data set contains:\n","\n","- 30 images of Mila Kunis\n","- 30 images of Jesse Eisenberg\n","- 10 images of Michael Cera (who looks like Jesse Eisenberg)\n","- 10 images of Sarah Hyland (who looks like Mila Kunis).\n","\n","The test set contains 1816 examples. \n","\n","Our goal is to build a **classifier** for face recognition of Mila and Jesse. \n","## 0.0. Loading libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-12T13:18:06.432333Z","iopub.status.busy":"2022-04-12T13:18:06.431451Z","iopub.status.idle":"2022-04-12T13:18:06.838582Z","shell.execute_reply":"2022-04-12T13:18:06.837957Z","shell.execute_reply.started":"2022-04-12T13:18:06.432235Z"},"id":"tTXKLUfua3U4","papermill":{"duration":0.53071,"end_time":"2022-04-09T17:04:07.408685","exception":false,"start_time":"2022-04-09T17:04:06.877975","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","\n","import io # Input/Output Module\n","import os # OS interfaces\n","import cv2 # OpenCV package\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import dlib\n","from urllib import request # module for opening HTTP requests\n","from matplotlib import pyplot as plt # Plotting library"]},{"cell_type":"markdown","metadata":{"id":"svAC3tmZnzvM"},"source":["## 0.1. Loading data\n","The training set is many times smaller than the test set and this might strike you as odd, however, this is close to a **real world** scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T13:18:13.084189Z","iopub.status.busy":"2022-04-12T13:18:13.083686Z","iopub.status.idle":"2022-04-12T13:18:34.123953Z","shell.execute_reply":"2022-04-12T13:18:34.123171Z","shell.execute_reply.started":"2022-04-12T13:18:13.084155Z"},"id":"_3Vq3yKha3U-","outputId":"6054c73f-bfad-4b72-e2e2-d486362e5197","trusted":true},"outputs":[],"source":["# Input data files are available in the read-only \"../input/\" directory\n","\n","train = pd.read_csv(\n","    '/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/train_set.csv', index_col = 0)\n","train.index = train.index.rename('id')\n","\n","test = pd.read_csv(\n","    '/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/test_set.csv', index_col = 0)\n","test.index = test.index.rename('id')\n","\n","# read the images as numpy arrays and store in \"img\" column\n","train['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/train/train_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n","                for index, row in train.iterrows()]\n","\n","test['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/test/test_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n","                for index, row in test.iterrows()]\n","  \n","\n","train_size, test_size = len(train),len(test)\n","\n","\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)\n","\n","# Input data files are available in the read-only \"../input/\" directory"]},{"cell_type":"markdown","metadata":{"id":"YTiSUsZFa3U_","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["*Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/).\n","\n","## 0.2. A first look\n","Let's have a look at the data columns and class distribution.\n","\n","The train set is a panda.Dataframe where each sample has four attributes: 'id', 'name', 'class', 'img'. The classes are:\n","\n","- Jesse: 1\n","- Mila: 2\n","- Michael or Sarah: 0\n","\n","The images are all RBG and have random sizes. At this point, we already understand that resizing all the images and working in monochrome will be helpful.  "]},{"cell_type":"markdown","metadata":{"id":"EaXdf6BnqSfn"},"source":["<img src=\"https://drive.google.com/uc?id=1fhZ61sk96xQS82BxoQhFNSsijGIa5CGx \" width=\"200\"/> <img src=\"https://drive.google.com/uc?id=1AtkvZr1BfqzehjSkgZUcITW5-j_g3FW_\" width=\"200\"/> <img src=\"https://drive.google.com/uc?id=1_a6D6nyWgkLwZ4tHd4HgPI0UYb-1LeVD\" width=\"200\"/> <img src=\"https://drive.google.com/uc?id=1h-1YsdFlDVzj65aWTTjoREj0ZQxhCQDZ\" width=\"200\"/> \n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:43:21.273759Z","iopub.status.busy":"2022-04-12T09:43:21.273287Z","iopub.status.idle":"2022-04-12T09:43:21.296959Z","shell.execute_reply":"2022-04-12T09:43:21.296049Z","shell.execute_reply.started":"2022-04-12T09:43:21.273717Z"},"id":"dxchOuRuje3u","outputId":"467405a0-bec8-495c-ce46-b4f3ae38178e","trusted":true},"outputs":[],"source":["im = train['img'][0]\n","imRGB = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n","cv2.imwrite('./im2.png', imRGB)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:43:28.282917Z","iopub.status.busy":"2022-04-12T09:43:28.282617Z","iopub.status.idle":"2022-04-12T09:43:44.202825Z","shell.execute_reply":"2022-04-12T09:43:44.202004Z","shell.execute_reply.started":"2022-04-12T09:43:28.282878Z"},"id":"HweJh7Boe7jq","outputId":"319a5d6e-bcda-488b-88cd-c53e72564422","trusted":true},"outputs":[],"source":["train.keys"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T07:38:26.069447Z","iopub.status.busy":"2022-04-12T07:38:26.069088Z","iopub.status.idle":"2022-04-12T07:38:26.077012Z","shell.execute_reply":"2022-04-12T07:38:26.076203Z","shell.execute_reply.started":"2022-04-12T07:38:26.069356Z"},"id":"pe-zMjM7kibK","outputId":"99685b1d-5f79-41e2-d559-547a4dde425b"},"outputs":[],"source":["print(train['name'].to_string())"]},{"cell_type":"markdown","metadata":{"id":"4vL6cKEXa3VA","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["## 0.3. Preprocess data\n","### 0.3.1 Example: dlib face detector\n","\n","\n","<!-- <div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You can write temporary files to <code>/kaggle/temp/</code> or <code>../../tmp</code>, but they won't be saved outside of the current session\n","</div> -->\n"]},{"cell_type":"markdown","metadata":{"id":"ebEIIh06a3VB","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["An important optimization in the pipeline is preprocessing. It makes sure that our raw sample images are formatted to agree on a number of properties;\n","\n","1. Fixed resolution\n","2. Normalized lighting\n","3. Aligned eyes (by rotation)\n","4. Fixed eye positions (by translation and scaling)\n","\n","To achieve these properties, the preprocessing performs the following steps:\n","\n","1. Convert the image to grayscale.\n","2. Try to detect faces in the image. If it does not contain exactly one face, assign '0'.\n","3. Translation & rotation\n","4. Light normalization & face cropping\n","\n","This process is shown by two example pictures below. On the left you see the original raw image. On the right you the output of the preprocessing. After the preprocessing we filter out the 'imposter' images from the training batch, that is all images which got assigned '0'.\n","\n","The face-and eye detection package that was used can be found here:\n","https://towardsdatascience.com/facial-mapping-landmarks-with-dlib-python-160abcf7d672"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T13:18:40.835800Z","iopub.status.busy":"2022-04-12T13:18:40.835484Z","iopub.status.idle":"2022-04-12T13:18:52.445846Z","shell.execute_reply":"2022-04-12T13:18:52.444654Z","shell.execute_reply.started":"2022-04-12T13:18:40.835766Z"},"id":"ReAH-lyja3VC","outputId":"657aee9c-d9d0-4bfb-c0bf-17a947d2cf70","trusted":true},"outputs":[],"source":["from urllib.request import urlretrieve\n","import bz2, shutil\n","\n","# Load file used to detect facial landmarks\n","url = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n","filename = 'shape_predictor_68_face_landmarks.dat.bz2'\n","\n","def download(url, file):\n","    print(os.path.isfile(file))\n","    if not os.path.isfile(file):\n","        print(\"Download file... \" + file + \" ...\")\n","        urlretrieve(url,file)\n","        print(\"File downloaded\")\n","\n","file = download(url,filename)\n","\n","\n","with bz2.BZ2File(filename) as input:\n","    with open('shape_predictor_68_face_landmarks.dat', 'wb') as output:\n","        shutil.copyfileobj(input, output)\n","  "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T13:20:09.836407Z","iopub.status.busy":"2022-04-12T13:20:09.836115Z","iopub.status.idle":"2022-04-12T13:20:09.915591Z","shell.execute_reply":"2022-04-12T13:20:09.914917Z","shell.execute_reply.started":"2022-04-12T13:20:09.836373Z"},"id":"ReAH-lyja3VC","outputId":"657aee9c-d9d0-4bfb-c0bf-17a947d2cf70","trusted":true},"outputs":[],"source":["      \n","url_2 = 'http://dlib.net/files/mmod_human_face_detector.dat.bz2'\n","filename_2 = 'mmod_human_face_detector.dat.bz2'\n","file_2 = download(url_2,filename_2)\n","\n","with bz2.BZ2File(filename_2) as input:\n","    with open('mmod_human_face_detector.dat', 'wb') as output:\n","        shutil.copyfileobj(input, output)"]},{"cell_type":"markdown","metadata":{},"source":["Let us choose if we will work with Gray or RGB images"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T13:21:19.561663Z","iopub.status.busy":"2022-04-12T13:21:19.560816Z","iopub.status.idle":"2022-04-12T13:21:19.565226Z","shell.execute_reply":"2022-04-12T13:21:19.564504Z","shell.execute_reply.started":"2022-04-12T13:21:19.561585Z"},"trusted":true},"outputs":[],"source":["gray_var = True\n","channel = None # if channel = -1 --> RGB images, if channel = None --> gray images"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T13:24:40.288285Z","iopub.status.busy":"2022-04-12T13:24:40.287512Z","iopub.status.idle":"2022-04-12T13:24:41.354516Z","shell.execute_reply":"2022-04-12T13:24:41.353554Z","shell.execute_reply.started":"2022-04-12T13:24:40.288232Z"},"id":"BGBysxv_a3VC","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["import matplotlib.patches as patches\n","from scipy import ndimage\n","import math\n","\n","l_eye_loc = 0.3\n","\n","# detector = dlib.get_frontal_face_detector()\n","detector = dlib.cnn_face_detection_model_v1('mmod_human_face_detector.dat')\n","predictor=dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n","\n","def preprocess(img, size, Gray, plot):\n","    norm = 100\n","    gray = cv2.cvtColor(np.uint8(img), cv2.COLOR_RGB2GRAY)\n","    \n","    if Gray == False:\n","        gray = img\n","        norm = 255\n","\n","    faces = [convert_and_trim_bb(image, r) for r in faces]\n","    if len(faces) ==0:\n","        return cv2.resize(gray,(size,size),interpolation=cv2.INTER_LINEAR ), 0\n","    \n","    if len(faces) >1:\n","        return cv2.resize(gray,(size,size),interpolation=cv2.INTER_LINEAR), 2\n","    \n","    (x, y, w, h) = faces[0].left(), faces[0].top(), faces[0].width(), faces[0].height()\n","    shape=predictor(gray,faces[0])\n","    \n","    #eyes \n","    #right eye, list of all landmarks, then compute center of mass\n","    re_xvalues = []\n","    re_yvalues=[]\n","    for i in range(36,42):\n","        re_xvalues.append(shape.part(i).x)\n","        re_yvalues.append(shape.part(i).y)\n","        \n","    le_xvalues = []\n","    le_yvalues=[]\n","    for i in range(42,48):\n","        le_xvalues.append(shape.part(i).x)\n","        le_yvalues.append(shape.part(i).y)     \n","        \n","    #center of mass\n","    left_eye = [np.mean(le_xvalues), np.mean(le_yvalues)]\n","    right_eye = [np.mean(re_xvalues), np.mean(re_yvalues)]\n","    \n","    dX=left_eye[0]-right_eye[0]\n","    dY = left_eye[1]-right_eye[1]\n","    angle = np.arctan(dY/dX)\n","        \n","    #calculate translation\n","    r_eye_loc = 1-l_eye_loc\n","    \n","    dist = np.sqrt((dX ** 2) + (dY ** 2))\n","    desiredDist = r_eye_loc-l_eye_loc\n","    desiredDist *= 256\n","    scale = desiredDist / dist\n","    \n","    eye_center = ((left_eye[0]+right_eye[0])//2 , (left_eye[1]+right_eye[1])//2)\n","    \n","    M = cv2.getRotationMatrix2D(eye_center, math.degrees(angle), scale)\n","    \n","    tX = 256 * 0.5\n","    tY = 256 * l_eye_loc\n","    M[0, 2] += (tX - eye_center[0])\n","    M[1, 2] += (tY - eye_center[1])\n","    \n","    img_rotated = cv2.warpAffine(gray, M, (256, 256)) \n","\n","    \n","    #normalize\n","    norm_img = np.zeros((size, size))\n","    norm_img = cv2.normalize(img_rotated, norm_img, 0, norm, cv2.NORM_MINMAX)\n","    \n","    cropped_im= cv2.resize(norm_img,(size,size),interpolation=cv2.INTER_LINEAR )\n","    \n","    if plot:\n","        fig, (ax1, ax2 ) = plt.subplots(1, 2, figsize=(20,10))\n","        ax1.imshow(img)\n","        ax1.set_title('Raw Image', fontsize=18)\n","        ax2.imshow(cropped_im, cmap='gray')\n","        ax2.set_title('Preprocessed Image', fontsize=30)\n","    return cropped_im, 1"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T13:24:50.667081Z","iopub.status.busy":"2022-04-12T13:24:50.666799Z","iopub.status.idle":"2022-04-12T13:24:55.148961Z","shell.execute_reply":"2022-04-12T13:24:55.148133Z","shell.execute_reply.started":"2022-04-12T13:24:50.667050Z"},"id":"k7CKdixOa3VD","outputId":"0ca6ec2e-9df3-4d7b-b26f-2dd2d73d27cd","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["preprocess(train['img'][4], 100, gray_var, True)"]},{"cell_type":"markdown","metadata":{"id":"VK_d1H6ta3VE","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["To make full use of the given training set we continue for pictures on which there are more than one face recognized. Below is function that will add faces to the training set, we manually labelled these extra faces."]},{"cell_type":"code","execution_count":277,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:41:17.314277Z","iopub.status.busy":"2022-04-12T12:41:17.313785Z","iopub.status.idle":"2022-04-12T12:41:17.329541Z","shell.execute_reply":"2022-04-12T12:41:17.328921Z","shell.execute_reply.started":"2022-04-12T12:41:17.314222Z"},"trusted":true},"outputs":[],"source":["def preprocess_m(img, size,Gray):\n","    norm = 100\n","    gray = cv2.cvtColor(np.uint8(img), cv2.COLOR_RGB2GRAY)\n","    if Gray == False:\n","        gray = img\n","        norm = 255\n","        \n","    #face\n","    face_list = []\n","    faces = detector(gray, 1)\n","    for face in faces:\n","    \n","        (x, y, w, h) = face.left(), face.top(), face.width(), face.height()\n","        shape=predictor(gray,face)\n","        #eyes \n","        #right eye, list of all landmarks, then compute center of mass\n","        re_xvalues = []\n","        re_yvalues=[]\n","        for i in range(36,42):\n","            re_xvalues.append(shape.part(i).x)\n","            re_yvalues.append(shape.part(i).y)\n","            \n","        le_xvalues = []\n","        le_yvalues=[]\n","        for i in range(42,48):\n","            le_xvalues.append(shape.part(i).x)\n","            le_yvalues.append(shape.part(i).y)     \n","            \n","        #center of mass\n","        left_eye = [np.mean(le_xvalues), np.mean(le_yvalues)]\n","        right_eye = [np.mean(re_xvalues), np.mean(re_yvalues)]\n","        \n","        dX=left_eye[0]-right_eye[0]\n","        dY = left_eye[1]-right_eye[1]\n","        angle = np.arctan(dY/dX)\n","        \n","        #calculate translation\n","        r_eye_loc = 1-l_eye_loc\n","        \n","        dist = np.sqrt((dX ** 2) + (dY ** 2))\n","        desiredDist = r_eye_loc-l_eye_loc\n","        desiredDist *= 256\n","        scale = desiredDist / dist\n","        \n","        eye_center = ((left_eye[0]+right_eye[0])//2 , (left_eye[1]+right_eye[1])//2)\n","        \n","        M = cv2.getRotationMatrix2D(eye_center, math.degrees(angle), scale)\n","        \n","        tX = 256 * 0.5\n","        tY = 256 * l_eye_loc\n","        M[0, 2] += (tX - eye_center[0])\n","        M[1, 2] += (tY - eye_center[1])\n","        \n","        img_rotated = cv2.warpAffine(gray, M, (256, 256)) \n","        \n","        #normalize\n","        norm_img = np.zeros((size, size))\n","        norm_img = cv2.normalize(img_rotated, norm_img, 0, norm, cv2.NORM_MINMAX)\n","        \n","        cropped_im= cv2.resize(norm_img,(size,size),interpolation=cv2.INTER_LINEAR )\n","        face_list.append(cropped_im)\n","    return face_list"]},{"cell_type":"markdown","metadata":{},"source":["Some plotting functions"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:49:19.912246Z","iopub.status.busy":"2022-04-12T09:49:19.911943Z","iopub.status.idle":"2022-04-12T09:49:19.917895Z","shell.execute_reply":"2022-04-12T09:49:19.916848Z","shell.execute_reply.started":"2022-04-12T09:49:19.912203Z"},"trusted":true},"outputs":[],"source":["def plot_image_sequence(data, imgs_per_column, imgs_per_row):\n","    fig = plt.figure(figsize=(100, 100))\n","    columns = imgs_per_row\n","    rows = imgs_per_column\n","    for i in range(len(data)):\n","        img = data[i]\n","        fig.add_subplot(rows, columns, i+1)\n","        plt.imshow(img)\n","    plt.show()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:49:21.717032Z","iopub.status.busy":"2022-04-12T09:49:21.716387Z","iopub.status.idle":"2022-04-12T09:49:21.725531Z","shell.execute_reply":"2022-04-12T09:49:21.724840Z","shell.execute_reply.started":"2022-04-12T09:49:21.716991Z"},"id":"fC_qntAQa3VF","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["def plot_image_sequence_advanced(data, n, imgs_per_row=7):\n","    n_rows = 1 + int(n/(imgs_per_row+1))\n","    n_cols = min(imgs_per_row, n)\n","\n","    f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n","    for i in range(n):\n","        if n == 1:\n","            ax.imshow(data[i],cmap='gray')\n","        elif n_rows > 1:\n","            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i],cmap='gray')\n","        else:\n","            ax[int(i%n)].imshow(data[i],cmap='gray')\n","    plt.show()"]},{"cell_type":"code","execution_count":296,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:48:52.597894Z","iopub.status.busy":"2022-04-12T12:48:52.597515Z","iopub.status.idle":"2022-04-12T12:49:15.240988Z","shell.execute_reply":"2022-04-12T12:49:15.240191Z","shell.execute_reply.started":"2022-04-12T12:48:52.597843Z"},"id":"tPKo1tuAa3VG","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["# Preprocessed data \n","\n","number_of_test_samples = 100\n","size=100\n","train_X_pr = [preprocess(img,size, gray_var, False) for img in train['img']]\n","test_X_pr = [preprocess(img,size, gray_var, False) for img in test['img'][:number_of_test_samples]]\n","\n","# Collect all train class label\n","train_y = np.array([train['class'].values[i] for i in range(len(train['class'].values))\n","           if train_X_pr[i][1]==1])\n","\n","# Based on labeling only go further with pictures with only one face on it\n","train_X = np.array([img[0] for img in train_X_pr if img[1]==1]) # If img[1] == 1 --> one face, img[1] == 2 --> more than 1 face, img[1] == 0 --> 0 faces\n","\n","# Collect all test images\n","test_X = [img[0] for img in test_X_pr]"]},{"cell_type":"markdown","metadata":{},"source":["Handcrafted test label list of first 100 samples to test accuracy at the end"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:51:07.461083Z","iopub.status.busy":"2022-04-12T09:51:07.460776Z","iopub.status.idle":"2022-04-12T09:51:07.469327Z","shell.execute_reply":"2022-04-12T09:51:07.468674Z","shell.execute_reply.started":"2022-04-12T09:51:07.461054Z"},"trusted":true},"outputs":[],"source":["test_y_acc =[1,0,0,0,1,1,1,0,1,0,2,1,2,0,0,1,1,0,0,1,2,2,2,1,0,1,0,0,0,2,2,1,0,0,2,1,1,0,0,1,0,1,1,1,0,1,1,0,0,0,1,0,0,0,2,0,2,0,0,1,2,0,0,2,0,0,0,0,1,0,0,2,0,0,1,1,0,2,1,0,0,0,2,0,0,2,0,1,2,2,0,0,0,0,2,0,0,0,1,2]"]},{"cell_type":"markdown","metadata":{},"source":["Replace class 0 by two new classes: 3 = Michael (--> g_ind), 4 = Sarah (--> f_ind)"]},{"cell_type":"markdown","metadata":{},"source":["The goal of creating this new classes is to increase the accuracy"]},{"cell_type":"code","execution_count":297,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:49:18.498676Z","iopub.status.busy":"2022-04-12T12:49:18.498344Z","iopub.status.idle":"2022-04-12T12:49:18.509489Z","shell.execute_reply":"2022-04-12T12:49:18.508675Z","shell.execute_reply.started":"2022-04-12T12:49:18.498642Z"},"trusted":true},"outputs":[],"source":["# Two different sets of list depending if we are using color or grayscale images (because with color images, the algorithm detect one more multiple faces picture)\n","if gray_var:\n","    g_ind = [2, 4, 15, 16, 24, 39, 58, 63, 65]\n","    f_ind = [11,20,32, 37, 42, 46,49,52]\n","else:\n","    g_ind = [2, 4, 15, 16, 24, 38, 57, 62, 64]\n","    f_ind = [11,20,31, 36, 41, 45,48,51]\n","\n","# Let us updates the label with the new classes\n","updated_labels=train_y\n","\n","for i in g_ind:\n","    updated_labels[i]=3\n","for i in f_ind:\n","    updated_labels[i]=4"]},{"cell_type":"markdown","metadata":{},"source":["We now create new datasets with the new classes and we add the new figures found on multiple faces picture to increase the train dataset"]},{"cell_type":"code","execution_count":307,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:52:38.821921Z","iopub.status.busy":"2022-04-12T12:52:38.821563Z","iopub.status.idle":"2022-04-12T12:52:38.832366Z","shell.execute_reply":"2022-04-12T12:52:38.831430Z","shell.execute_reply.started":"2022-04-12T12:52:38.821883Z"},"trusted":true},"outputs":[],"source":["# Add data of multiple faces images to training set (we have thanks to this, 19 new pictures added to the train dataset)\n","\n","\n","if not gray_var:\n","    train_X_full = np.zeros((len(updated_labels) + 19, size, size,3)).astype(int)\n","    train_y_full = np.zeros(len(updated_labels) + 19)\n","    train_y_full[:len(updated_labels)] = updated_labels\n","else:\n","    train_X_full = np.zeros((len(updated_labels) + 17, size, size)).astype(int)\n","    train_X_full[:len(updated_labels)] = train_X\n","    train_y_full = np.zeros(len(updated_labels) + 17)\n","    train_y_full[:len(updated_labels)] = updated_labels"]},{"cell_type":"markdown","metadata":{},"source":["Let us now add these new pictures to the training dataset. We checked each original multiple faces picture index to extract them more easily"]},{"cell_type":"code","execution_count":308,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:52:41.882172Z","iopub.status.busy":"2022-04-12T12:52:41.881856Z","iopub.status.idle":"2022-04-12T12:52:43.812341Z","shell.execute_reply":"2022-04-12T12:52:43.811156Z","shell.execute_reply.started":"2022-04-12T12:52:41.882139Z"},"trusted":true},"outputs":[],"source":["# We add those new faces to the train dataset with their corresponding label\n","\n","for i, el in enumerate(train_X_pr):\n","    if i==18:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)] = r[0]\n","        train_y_full[len(train_X)] = 3\n","        \n","        train_X_full[len(train_X)+1] = r[1]\n","        train_y_full[len(train_X)+1] = 1\n","    elif i==28:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+2] = r[0]\n","        train_y_full[len(train_X)+2] = 2\n","    elif i==30:\n","        # Multiple faces picture not detected when we work with grayscale images\n","        if not gray_var:\n","            r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","            train_X_full[len(train_X)+17] = r[0]\n","            train_y_full[len(train_X)+17] = 2\n","\n","            train_X_full[len(train_X)+18] = r[1]\n","            train_y_full[len(train_X)+18] = 2\n","    elif i==32:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+3] = r[0]\n","        train_y_full[len(train_X)+3] = 4\n","        \n","        train_X_full[len(train_X)+4] = r[1]\n","        train_y_full[len(train_X)+4] = 4\n","    elif i==34:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+5] = r[1]\n","        train_y_full[len(train_X)+5] = 3\n","        \n","        train_X_full[len(train_X)+6] = r[2]\n","        train_y_full[len(train_X)+6] = 1\n","        \n","    elif i==41:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+7] = r[1]\n","        train_y_full[len(train_X)+7] = 3\n","        \n","    elif i==49:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+8] = r[1]\n","        train_y_full[len(train_X)+8] = 1\n","        \n","    elif i==50:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+9] = r[0]\n","        train_y_full[len(train_X)+9] = 2\n","        \n","    elif i==52:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+10] = r[1]\n","        train_y_full[len(train_X)+10] = 1\n","        \n","    elif i==53:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+11] = r[0]\n","        train_y_full[len(train_X)+11] = 1\n","        \n","    elif i==59:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+12] = r[0]\n","        train_y_full[len(train_X)+12] = 4\n","        \n","    elif i==61:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+13] = r[0]\n","        train_y_full[len(train_X)+13] = 2\n","        \n","    elif i==70:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+14] = r[1]\n","        train_y_full[len(train_X)+14] = 1\n","        \n","    elif i==77:\n","        r = preprocess_m(train['img'][i], size, Gray=gray_var)\n","        train_X_full[len(train_X)+15] = r[0]\n","        train_y_full[len(train_X)+15] = 2\n","        \n","        train_X_full[len(train_X)+16] = r[1]\n","        train_y_full[len(train_X)+16] = 2"]},{"cell_type":"markdown","metadata":{},"source":["Save indexes of no faces, one face and multiple faces pictures for the test dataset (it will be used latter to calculate the predictions)"]},{"cell_type":"code","execution_count":242,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:30:54.725930Z","iopub.status.busy":"2022-04-12T12:30:54.725413Z","iopub.status.idle":"2022-04-12T12:30:54.732515Z","shell.execute_reply":"2022-04-12T12:30:54.731919Z","shell.execute_reply.started":"2022-04-12T12:30:54.725877Z"},"id":"kKE2DNdna3VG","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["\n","# Screening of test data\n","\n","# Zero faced images\n","bool_list = np.array([img[1] for img in test_X_pr])==0\n","zeros_ind = [i for i, x in enumerate(bool_list) if x]\n","\n","# One-faced images\n","bool_list = np.array([img[1] for img in test_X_pr])==1\n","face_ind = [i for i, x in enumerate(bool_list) if x]\n","\n","# More than one-faced images\n","bool_list = np.array([img[1] for img in test_X_pr])==2\n","faces_ind = [i for i, x in enumerate(bool_list) if x]\n","\n","\n","# Final classes prediction of test dataset\n","labels = np.zeros(len(test_X_pr))"]},{"cell_type":"markdown","metadata":{},"source":["Let us increase our initial dataset with data_augmentation to improve our predictions"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:51:33.848828Z","iopub.status.busy":"2022-04-12T09:51:33.848558Z","iopub.status.idle":"2022-04-12T09:51:39.062737Z","shell.execute_reply":"2022-04-12T09:51:39.061889Z","shell.execute_reply.started":"2022-04-12T09:51:33.848798Z"},"trusted":true},"outputs":[],"source":["# import library for data augmentation\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":109,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T11:26:46.791074Z","iopub.status.busy":"2022-04-12T11:26:46.790222Z","iopub.status.idle":"2022-04-12T11:26:46.801738Z","shell.execute_reply":"2022-04-12T11:26:46.801081Z","shell.execute_reply.started":"2022-04-12T11:26:46.791031Z"},"trusted":true},"outputs":[],"source":["def data_augmentation_func(images, classes):\n","    \"\"\"\n","    Increase the initial train dataset\n","\n","    Input: initial train dataset\n","    Output: augmented train dataset\n","    \"\"\"\n","    \n","    augmented_images = images.tolist()\n","    augmented_classes = classes.tolist()\n","\n","    for i, image in enumerate(images):\n","        if not gray_var:\n","            # Let us add grayscale images of the initial train dataset\n","            image_ = tf.image.rgb_to_grayscale(image)\n","            image_ = tf.squeeze(image_)\n","            image_ = cv2.cvtColor(np.uint8(image_),cv2.COLOR_GRAY2RGB)\n","        else:\n","            # If we are working with grayscale images let use add some saturated images\n","            image_ = tf.image.adjust_saturation(image, 3)\n","        augmented_images.append(image_)\n","        augmented_classes.append(classes[i])\n","        \n","        # Let us add pictures with brightness changes of the initial train dataset\n","        bright = tf.image.adjust_brightness(image, 0.4)\n","        augmented_images.append(bright)\n","        augmented_classes.append(classes[i])\n","        \n","        # Let us add some saturated images\n","        saturated = tf.image.adjust_saturation(image, 3)\n","        augmented_images.append(saturated)\n","        augmented_classes.append(classes[i])\n","    return [np.asarray(augmented_images), augmented_classes]"]},{"cell_type":"markdown","metadata":{},"source":["We notice that we had better performances by adding grayscale images and brighter images to the initial train dataset"]},{"cell_type":"markdown","metadata":{},"source":["Let us now create our new augmented train dataset"]},{"cell_type":"code","execution_count":125,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T11:30:23.353379Z","iopub.status.busy":"2022-04-12T11:30:23.353085Z","iopub.status.idle":"2022-04-12T11:30:25.625924Z","shell.execute_reply":"2022-04-12T11:30:25.624962Z","shell.execute_reply.started":"2022-04-12T11:30:23.353349Z"},"trusted":true},"outputs":[],"source":["output = data_augmentation_func(train_X_full,train_y_full)\n","train_X_full_trans = output[0]\n","train_y_full_trans = output[1]"]},{"cell_type":"markdown","metadata":{"id":"n4K0If_9a3VI","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["# 1. Feature Representations\n","\n","In this section, we will explore two methods to extract features from images. As we're concerned with processing faces, these features include various facial properties such as eyes, mouths, noses, ears, etc. We will look at both handcrafted features and learned features.\n","\n","Handcrafted feature descriptors are extracted from the image itself, using a specific deterministic algorithm that might differ from task to task. In this assignment we will look at Histogram of Gradients (HOG). Alternatively, we will look at learned features by using Principal Components Analysis (PCA). In this case, the algorithm learns the features from analysing the training data."]},{"cell_type":"markdown","metadata":{"id":"C--fPhVQa3VJ","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["## 1.1. Handcrafted Features: Histogram of Gradients (HOG)\n","\n","For the handcrafted features, we opted for the HOG feature descriptor. We tried out both HOG and SIFT, but concluded that HOG yields better results.\n","\n","First, HOG devides a given picture into connection regions called \"cells\". For the pixels within each cell, the algorithm compiles a histogram of gradient directions. The descriptor is the concatenation of all these histograms. Ideally, we want our descriptor to be invariant with regards to illumination and shadowing. To achieve this, we can use a process called contrast-normalization. In short, we group multiple cells together in \"blocks\" for which we calculate a measure of the intensity. This value is then used to normalize all cells within said block. So for an input image of size 64 x 128 x 3, HOG will output a feature vector with a length of 3780.\n","\n","For more information cfr. https://learnopencv.com/histogram-of-oriented-gradients/.\n","\n","\n"]},{"cell_type":"code","execution_count":311,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:54:55.271008Z","iopub.status.busy":"2022-04-12T12:54:55.270620Z","iopub.status.idle":"2022-04-12T12:54:55.464345Z","shell.execute_reply":"2022-04-12T12:54:55.463407Z","shell.execute_reply.started":"2022-04-12T12:54:55.270965Z"},"trusted":true},"outputs":[],"source":["#step one devide the face into cells\n","cell_size = 10\n","if gray_var:\n","    image_c = cv2.cvtColor(np.uint8(train_X_full[0]), cv2.COLOR_GRAY2RGB)\n","else:\n","    image_c =train_X_full[0]\n","def drawcells(image, dx, dy, grid_color):\n","    img = image.copy()\n","    # Modify the image to include the grid\n","    img[:,::dy,:] = grid_color\n","    img[::dx,:,:] = grid_color\n","    return img\n","\n","\n","cell_ = drawcells(image_c, cell_size,cell_size,[0,255,0])\n","plt.imshow(cell_)\n","plt.show()"]},{"cell_type":"code","execution_count":312,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:54:56.850400Z","iopub.status.busy":"2022-04-12T12:54:56.849690Z","iopub.status.idle":"2022-04-12T12:54:57.073486Z","shell.execute_reply":"2022-04-12T12:54:57.072415Z","shell.execute_reply.started":"2022-04-12T12:54:56.850362Z"},"trusted":true},"outputs":[],"source":["#create a histogram for the first cell, for simplicity use grayscale\n","\n","#calculate magnitude and angle of each pixel, ksize=1\n","def get(X):\n","    im = X\n","\n","    # Calculate gradient\n","    gx = cv2.Sobel(im, cv2.CV_32F, 1, 0, ksize=1)\n","    gy = cv2.Sobel(im, cv2.CV_32F, 0, 1, ksize=1)\n","    mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n","    return mag, angle\n","\n","if gray_var:\n","    image = np.uint8(train_X_full[0] )\n","else:\n","    image = cv2.cvtColor(np.uint8(train_X_full[0]), cv2.COLOR_RGB2GRAY)\n","\n","def find_index(array, el):\n","    for i,l in enumerate(array):\n","        if l>el:\n","            break\n","    return i\n","\n","mag, angle = get(image)\n","\n","# the gradient should not differentiate between low->high and high->low\n","angle = angle % 180\n","hist_bins = np.array([10,30,50,70,90,110,130,150,170])\n","\n","cell_angles = angle[0:cell_size,0:cell_size]\n","cell_magnitudes = mag[0:cell_size,0:cell_size]\n","\n","hist_values = np.zeros(len(hist_bins))\n","\n","for row_idx in range(cell_size):\n","    for col_idx in range(cell_size):\n","        angle_ = cell_angles[row_idx, col_idx]\n","        magn_ = cell_magnitudes[row_idx, col_idx]\n","        i = find_index(hist_bins,angle_)\n","        hist_values[i]+=magn_\n","\n","plt.bar(x=hist_bins, height=hist_values, align=\"center\", width=15)\n","plt.title('Histogram of first cell')\n","plt.xlabel('Angle Bins')\n","plt.ylabel('Cum Magnitude')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Ofcourse the values need to be between 0 and 1 so a normalization is required, we could do this by just normalizing this vector or we could take 3 neighbouring vectors as follows and normalize over all 4 of them:"]},{"cell_type":"code","execution_count":313,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:54:59.814425Z","iopub.status.busy":"2022-04-12T12:54:59.814143Z","iopub.status.idle":"2022-04-12T12:55:00.005572Z","shell.execute_reply":"2022-04-12T12:55:00.004822Z","shell.execute_reply.started":"2022-04-12T12:54:59.814396Z"},"trusted":true},"outputs":[],"source":["cell__ = drawcells(cell_, cell_size*2,cell_size*2,[255,0,0])\n","plt.imshow(cell__)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The HOG is calculated on the data shown in the image below, not the picture above."]},{"cell_type":"code","execution_count":314,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:55:01.780042Z","iopub.status.busy":"2022-04-12T12:55:01.779508Z","iopub.status.idle":"2022-04-12T12:55:01.995131Z","shell.execute_reply":"2022-04-12T12:55:01.994186Z","shell.execute_reply.started":"2022-04-12T12:55:01.780005Z"},"trusted":true},"outputs":[],"source":["pixels = 10\n","\n","fd, hog_image = hog(image_c, orientations=9, pixels_per_cell=(cell_size, cell_size),\n","                    cells_per_block=(1, 1), visualize=True, channel_axis=-1)\n","plt.imshow(hog_image, cmap='gray')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["All the normalized feature vectors of each red block is then concatenated into one big vector which results in the feature vector."]},{"cell_type":"markdown","metadata":{},"source":["Let us create the hog features thanks to the skimage library and then visualize then with t-distributed stochastic neighbour embedding (T-SNE). T-SNE is a way to reduce multi-dimensional data. Clustering occers on our train data set, this is a good indication that we can continue with this feature representation."]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T10:45:22.601943Z","iopub.status.busy":"2022-04-12T10:45:22.601554Z","iopub.status.idle":"2022-04-12T10:45:22.606713Z","shell.execute_reply":"2022-04-12T10:45:22.605923Z","shell.execute_reply.started":"2022-04-12T10:45:22.601904Z"},"trusted":true},"outputs":[],"source":["#T-SNE\n","from skimage.feature import hog\n","from sklearn.manifold import TSNE\n","import seaborn as sns"]},{"cell_type":"code","execution_count":290,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:44:31.370769Z","iopub.status.busy":"2022-04-12T12:44:31.370232Z","iopub.status.idle":"2022-04-12T12:44:31.375486Z","shell.execute_reply":"2022-04-12T12:44:31.374898Z","shell.execute_reply.started":"2022-04-12T12:44:31.370723Z"},"trusted":true},"outputs":[],"source":["len(train_y_full)"]},{"cell_type":"code","execution_count":291,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:44:53.899492Z","iopub.status.busy":"2022-04-12T12:44:53.898915Z","iopub.status.idle":"2022-04-12T12:44:53.904787Z","shell.execute_reply":"2022-04-12T12:44:53.903754Z","shell.execute_reply.started":"2022-04-12T12:44:53.899454Z"},"trusted":true},"outputs":[],"source":["len(train_X_full)"]},{"cell_type":"code","execution_count":315,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:55:05.270224Z","iopub.status.busy":"2022-04-12T12:55:05.269918Z","iopub.status.idle":"2022-04-12T12:55:06.499111Z","shell.execute_reply":"2022-04-12T12:55:06.498205Z","shell.execute_reply.started":"2022-04-12T12:55:05.270194Z"},"id":"mJkaalVia3VL","outputId":"949c80d2-a040-4a9f-83a2-aa9edddbfb97","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["pixels = 10\n","hog_df=pd.DataFrame()\n","hog_df['img']=list(train_X_full)\n","hog_df['class']=train_y_full\n","fvs = [None]*len(train_X_full)\n","for i, img in enumerate(hog_df['img']):\n","    if gray_var:\n","        fv = list(hog(img, orientations=9, pixels_per_cell=(pixels,pixels), block_norm='L2',\n","                        cells_per_block=(2, 2), feature_vector='True'))\n","    else:\n","        fv = list(hog(img, orientations=9, pixels_per_cell=(pixels,pixels), block_norm='L2',\n","                        cells_per_block=(2, 2), feature_vector='True', channel_axis =1))\n","    fvs[i] = fv\n","hog_df['fv']= fvs\n","\n","SNE_data = pd.DataFrame()\n","X_embedded = TSNE(n_components=2, learning_rate=200,\n","                  init='random', random_state =0).fit_transform(fvs)\n","\n","#normalization\n","SNE_data['x']= X_embedded[:,0]+abs(min(X_embedded[:,0]))\n","SNE_data['X']=SNE_data['x']/max(SNE_data['x'])\n","SNE_data['y']=X_embedded[:,1]+abs(min(X_embedded[:,1]))\n","SNE_data['Y']=SNE_data['y']/max(SNE_data['y'])\n","SNE_data['label']= hog_df['class']\n","\n","fig, ax = plt.subplots(figsize=(16,10))\n","sns.scatterplot(x='X', y='Y', data =SNE_data,palette=sns.color_palette(\"hls\", 4), hue='label')"]},{"cell_type":"markdown","metadata":{"id":"2y1NJFB-a3VM","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["### 1.1.2. HOG: Discussion\n","\n","**Hyperparameters**\n","\n","**How to make your descriptor behave well in different circumstances?**\n","\n","Circumstances can lead to slight perturbations in the look on a person's face, such as:\n","- Background\n","-\tFace partially covered by objects\n","- Facial expression\n","-\tGeneral image quality\n","-\tLighting\n","- Make-up and hair style\n","-\tRotation\n","-\tScale\n","\n","Some of these perturbations, such as rotation and scale, are taken care of by the preprocessing. Lighting is normalized during the final step of HOG as explained earlier.\n","\n","**How does this feature compare to your previous grabbing task in the individual assignment?**\n","\n","The individual assignment requested object grabbing in binary space, with the forground object in white and the background in black. This operation required specific parametrization, depending on the colors of the objects in the frame. Thus, it is only viable in a controlled environment where the luminosity remains constant.\n","\n","HOG on the other hand can handle complex and multicolor objects rather well. It has proven to be robust both to noise and lighting variations. While both methods can perform object detection, HOG seems to be more reliable.\n","\n","**Did you need specific pre-processing steps before computing these feature descriptors on your images (which ones and why)?**\n","\n","Yes, we need to make sure that each picture has the same format. Meaning that the extracted faces need to have the same size and shape, and more important, that all faces are aligned. We do this by giving a specific location for the eyes. Then we rescale the face such that it becomas symmetric.\n","\n","**Did the visualisation show good discriminative and robustness properties?**\n","\n","\n","In the T-SNE plot above we can clearly see discriminative clusters Ofcourse this is due the preproccesing of the images. If we would just do hog on raw pictures random background objects would influence the result significant. We saw that when the picture is cropped, small differentiation on the parameters of the location of the eyes or cell size did not change the visualization so starting from basic preprocessed data this technique is quite robust.\n"]},{"cell_type":"markdown","metadata":{"id":"KPBiMhECa3VM","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["## 1.2. Learned Features: Principal Component Analysis (PCA)"]},{"cell_type":"markdown","metadata":{"id":"RpkKOyfNouIv"},"source":["PCA is a technique to compress the data from a high dimensional space to a lower one, while keeping most of the variance present in the data. The high-dimensional data is projected on the eigenvectors of the data tensor, which we call the principal components (PCs). The animation below shows the data (blue points) being projected orthogonally on a component (rotating axis). At some point, the component captures maximally the variance in the data. Like that, the data has been compressed from 2 dimensions to 1 dimension. \n","\n","![alt text](https://miro.medium.com/max/700/1*XGaA7KWUlhWZLIezYEBIHA.gif)\n","\n","The PCs are uncorrelated and orthogonal vectors pointing towards where the variance is maximal in the high-dimensional data. The aim is to select the lowest number of PCs such that they are representative well enough the data. These features of the data should be both robust (the features should stay the same for different representations of the same object) and discriminative (the features allow to differentiate different objects).\n","\n","**PCA**\n","\n","PCA is based on the Eigenvalue Decomposition (EVD). Let covariance matrix $C$:\n","\n","$$ C = \\frac{X \\: X^T}{n-1} $$ \n","\n","where $X$ is the data matrix, $C$ is symmetric with shape $p$ x $p$ and hence always diagonalizable. We apply the EVD as follows:\n","\n","$$ C = W Λ W^T $$\n","\n","$W$ is an orthogonal matrix and its columns are the eigenvectors of $C$ and $Λ$ is a diagonal matrix containing the corresponding eigenvalues in decreasing order on the diagonal ($λ_1 > λ_2 > λ_3$..). The eigenvectors are the PCs we are seeking. \n","\n","**SVD** \n","\n","Instead of applying PCA to get the PCs, we performed Singular Values Decomposition (SVD) on the data. SVD is known to be more computationally efficient. \n","\n","Any matrix $X$ can be decomposed in the following:\n","\n","$$ X = U_{n \\:x \\:n} Σ_{n\\: x \\:p} V_{p\\: x \\:n}^T $$\n","\n","Both $U$ and $V$ are orthogonal and $Σ$ is diagonal. The diagonal elements of Σ are called singular values (σ₁ ≥ σ₂ ≥ … ≥ σₚ ≥ 0).\n","\n","When comparing SVD to EVD, we observe:\n","\n","$$ C = \\frac{X \\: X^T}{n-1} = \\frac{V Σ^T U^T U Σ V^T}{n-1} = V\\frac{Σ^2}{n-1}V^T $$ \n","\n","We can deduce that the columns of $V$ are the PCs and the i-th eigenvalue λᵢ = σᵢ²/(n-1).\n","\n","We use the svd(X_centered) function of numpy where X is the data matrix which has been centered beforehand. \n","\n","**PCA for face recognition**\n","\n","The PCs capture the features of the faces. We study the influence of the number of PCs used to described the data trhough an analysis of the recontruction error.  \n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:53:29.943839Z","iopub.status.busy":"2022-04-12T09:53:29.943572Z","iopub.status.idle":"2022-04-12T09:53:29.948525Z","shell.execute_reply":"2022-04-12T09:53:29.947571Z","shell.execute_reply.started":"2022-04-12T09:53:29.943812Z"},"id":"ROYrHNuDa3VN","trusted":true},"outputs":[],"source":["FACE_SIZE = (100, 100)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:53:33.420483Z","iopub.status.busy":"2022-04-12T09:53:33.419953Z","iopub.status.idle":"2022-04-12T09:53:33.431773Z","shell.execute_reply":"2022-04-12T09:53:33.430816Z","shell.execute_reply.started":"2022-04-12T09:53:33.420442Z"},"id":"u3SpV0yda3VN","trusted":true},"outputs":[],"source":["# Transform all images to an array in 2D (dim: dxd), then transform the array to a single vector of d*d elements \n","m_test = len(test_X)\n","\n","# Dimension of the images\n","d_test = FACE_SIZE[0]**2\n","\n","if not gray_var:\n","    X_gray_test = np.zeros((m_test, *FACE_SIZE))\n","\n","    # For each, transformation to a 2D array --> grayscale transformation\n","    for i, img in enumerate(test_X): \n","        X_gray_test[i, :] = cv2.cvtColor(np.float32(img), cv2.COLOR_RGB2GRAY).astype(int)\n","\n","    # Transform matrices to vectors\n","    X_test_flat = np.reshape(X_gray_test, (m_test, d_test))\n","else: \n","    X_test_flat = np.reshape(test_X, (m_test, d_test))"]},{"cell_type":"markdown","metadata":{},"source":["Let us define an identity feature extractor which returns the input"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:53:36.644607Z","iopub.status.busy":"2022-04-12T09:53:36.644003Z","iopub.status.idle":"2022-04-12T09:53:36.649261Z","shell.execute_reply":"2022-04-12T09:53:36.648362Z","shell.execute_reply.started":"2022-04-12T09:53:36.644568Z"},"id":"1iYFf4puUeaw","trusted":true},"outputs":[],"source":["class IdentityFeatureExtractor:\n","    \"\"\"A simple function that returns the input\"\"\"\n","    \n","    def transform(self, X):\n","        return X\n","    \n","    def __call__(self, X):\n","        return self.transform(X)"]},{"cell_type":"markdown","metadata":{},"source":["Let us define our PCAFeatureExctractor (another possibility, for example, would be to use PCA function from sklearn.decomposition)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:53:37.315164Z","iopub.status.busy":"2022-04-12T09:53:37.314844Z","iopub.status.idle":"2022-04-12T09:53:37.324023Z","shell.execute_reply":"2022-04-12T09:53:37.323074Z","shell.execute_reply.started":"2022-04-12T09:53:37.315130Z"},"id":"oUaLJQf2a3VO","trusted":true},"outputs":[],"source":["class PCAFeatureExtractor(IdentityFeatureExtractor):\n","    \"\"\"Extractor of principal components\"\"\"\n","    \n","    def __init__(self, n_components):\n","        \"\"\"initialization of variables\"\"\"\n","        self.n_components = n_components\n","        self.mean = None\n","        self.vt = None\n","        self.principal_components = None\n","    \n","    def transform(self, X):\n","        \"\"\"Get principal components from images\"\"\"\n","        self.mean = np.mean(X, axis=0, dtype=int) # Or just use that if not conversion to int: X.mean(axis=0)\n","        X_centered = X - self.mean # Centered image\n","        \n","        U, S, Vt = np.linalg.svd(X_centered) # Get SVD from images\n","        \n","        self.vt = Vt[:self.n_components, :] # Eigenvectors matrix\n","        \n","        self.principal_components = X_centered @ self.vt.T # Get principal components\n","                \n","        return self.principal_components\n","        \n","    def inverse_transform(self, X):\n","        \"\"\"Go back to initial space, back to initial images\"\"\"\n","        raise X @ self.vt - self.mean"]},{"cell_type":"markdown","metadata":{},"source":["Let us calculate the pca components for the train and the test dataset"]},{"cell_type":"code","execution_count":216,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:23:29.322252Z","iopub.status.busy":"2022-04-12T12:23:29.321901Z","iopub.status.idle":"2022-04-12T12:23:29.326672Z","shell.execute_reply":"2022-04-12T12:23:29.325752Z","shell.execute_reply.started":"2022-04-12T12:23:29.322215Z"},"id":"MjK0Km0Aa3VO","outputId":"b0078b7c-ae7b-4f10-f182-33b71aaf5b43","trusted":true},"outputs":[],"source":["# Init PCA Feature Extractor + calculate principal components\n","nbre_features = 10\n","pca_train = PCAFeatureExtractor(nbre_features)\n","pca_train.transform(X_flat)\n","\n","pca_test = PCAFeatureExtractor(nbre_features)\n","pca_test.transform(X_test_flat)"]},{"cell_type":"markdown","metadata":{"id":"2aE-35T8XLR8"},"source":["### 1.2.1. Eigenface Plots\n","\n","We look at a few eigenfaces. When looking at the first eigenface, we observe a very high contrast between the background and key traits of the human face like the eyes, eyebrows and mouth. This implies that the variance that is captured with this component mostly affects the background. The beauty of PCA is that we immediatly see what **features** the algorithm has learnt.\n","\n"," We note that later eigenfaces become more and more abstract and **less informative**."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:10:13.660889Z","iopub.status.busy":"2022-04-12T09:10:13.660552Z","iopub.status.idle":"2022-04-12T09:10:14.455567Z","shell.execute_reply":"2022-04-12T09:10:14.454580Z","shell.execute_reply.started":"2022-04-12T09:10:13.660842Z"},"id":"_FzFBMDsVel1"},"outputs":[],"source":["U, Sigma, VT = np.linalg.svd(X_flat, full_matrices=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:10:59.980337Z","iopub.status.busy":"2022-04-12T09:10:59.980024Z","iopub.status.idle":"2022-04-12T09:11:42.029428Z","shell.execute_reply":"2022-04-12T09:11:42.025115Z","shell.execute_reply.started":"2022-04-12T09:10:59.980309Z"},"id":"Rh43lgHNVZ3M","outputId":"152fa4eb-22ac-4892-8fa4-d8f5d1dd99c5"},"outputs":[],"source":["plt.set_cmap(plt.cm.gray)\n","plot_image_sequence_advanced(VT.reshape(VT.shape[0], *(100,100)), VT.shape[0], 10)"]},{"cell_type":"markdown","metadata":{"id":"d72_nc-jWIMb"},"source":["Hereafter we plot the **mean eigenface**. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:11:53.883840Z","iopub.status.busy":"2022-04-12T09:11:53.883029Z","iopub.status.idle":"2022-04-12T09:11:54.061427Z","shell.execute_reply":"2022-04-12T09:11:54.060325Z","shell.execute_reply.started":"2022-04-12T09:11:53.883806Z"},"id":"YJKq0UWlV_GR","outputId":"4169d5dd-61c4-45a2-bab7-4b0058232d96"},"outputs":[],"source":["mean_face = VT.mean(axis=0)\n","plt.imshow(mean_face.reshape((100,100)), cmap= plt.cm.gray)"]},{"cell_type":"markdown","metadata":{"id":"yIX8soqya3VO","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["### 1.2.2. Feature Space Plots\n","\n","In the example above, we performed PCA for all PCs. However, one key idea of PCA is dimensionality reduction. In this section, we study how to keep a minimal number of PCs while keeping most of the variance in the data. \n","\n","Hereafter we choose to project the image sample on the 3 first components (the components that explain the most the variance in the data). Therefore, a sample vector of dimension $100^2$ (number of pixel in an image) is reduced to only 3. The plot of the samples in the 3 PCs space is shown below. We observe that there is a separation in the data meaning that the first components seemed to have captured the features of Jesse and Mila.  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:11:59.047766Z","iopub.status.busy":"2022-04-12T09:11:59.046588Z","iopub.status.idle":"2022-04-12T09:11:59.055738Z","shell.execute_reply":"2022-04-12T09:11:59.054605Z","shell.execute_reply.started":"2022-04-12T09:11:59.047707Z"},"id":"msIbvHhea3VP"},"outputs":[],"source":["# Create a list of the names\n","\n","names = list()\n","for i in train_y_full_trans:\n","    if i == 1:\n","        names.append(\"Jesse\")\n","    elif i == 2:\n","        names.append(\"Mila\")\n","    else:\n","        names.append(\"Michael and Sarah\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:12:01.773021Z","iopub.status.busy":"2022-04-12T09:12:01.772627Z","iopub.status.idle":"2022-04-12T09:12:14.945282Z","shell.execute_reply":"2022-04-12T09:12:14.944287Z","shell.execute_reply.started":"2022-04-12T09:12:01.772984Z"},"id":"ENTtu8G4a3VP","outputId":"7076b44a-a882-4dd6-fb56-73878b7b4cef","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["# Plot the images in a 3D Feature space plot\n","\n","pca = PCAFeatureExtractor(3)\n","projected_3D = pca.transform(X_flat)\n","\n","names = np.asarray(names)\n","\n","\n","fig = plt.figure(figsize = (10, 10))\n","ax = plt.axes(projection =\"3d\")\n","\n","for name in np.unique(names): \n","    person = projected_3D[names == name]    \n","    ax.scatter3D(person[:, 0],\n","                 person[:, 1],\n","                 person[:, 2],\n","                 label= ' '.join(name.split('_'))\n","                )    \n","\n","ax.set_xlabel('PC 1')\n","ax.set_ylabel('PC 2')\n","ax.set_zlabel('PC 3')\n","ax.legend(loc=1)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"A6RJo2nOd2jo"},"source":["We do the same using only the first 2 components. We were able to project high-dimensional data into a 2-dimentional space. This projection shows us that the 2 first components are able to separate a man and a woman. However, our task is to do face recognition, and more PCs may be needed. \n","\n","The plot shows us that the data is splitted more along the x-axis, meaning that the **1st component** seems to explain more the variance."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:12:20.190314Z","iopub.status.busy":"2022-04-12T09:12:20.190028Z","iopub.status.idle":"2022-04-12T09:12:33.971315Z","shell.execute_reply":"2022-04-12T09:12:33.970300Z","shell.execute_reply.started":"2022-04-12T09:12:20.190284Z"},"id":"rRFqnOINa3VP","outputId":"38d371cc-3b2b-4c69-ea97-bb07da7cec92"},"outputs":[],"source":["# Plot the images in a 2D Feature space plot\n","\n","pca = PCAFeatureExtractor(2)\n","projected_2D = pca.transform(X_flat)\n","\n","names = np.asarray(names)\n","\n","ax = plt.axes()\n","\n","for name in np.unique(names): \n","    person = projected_2D[names == name]  \n","    plt.scatter(person[:, 0],person[:, 1],label= ' '.join(name.split('_')))\n","         \n","ax.set_xlabel('PC 1')\n","ax.set_ylabel('PC 2')\n","ax.legend(loc=1)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"3UZ8MSnXftcO"},"source":["### Explained variance and number $p$ of *PCs*\n","\n","The explained variance ratio is computed as:\n","\n","$$\\frac{\\Sigma^2}{\\Sigma \\sigma_{i}}$$\n","\n","We plot the explained variance ratio in function of the PCs (for the 49 first components). The Figure suggests that the 1st component has a very big weight with respect to the others. \n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:12:38.175636Z","iopub.status.busy":"2022-04-12T09:12:38.175341Z","iopub.status.idle":"2022-04-12T09:12:38.904718Z","shell.execute_reply":"2022-04-12T09:12:38.904087Z","shell.execute_reply.started":"2022-04-12T09:12:38.175606Z"},"id":"Jb6MhFJ7hFAM","outputId":"512f1e95-554b-476e-d63d-5b4488469948"},"outputs":[],"source":["explained_variance_ratio = Sigma ** 2 / (Sigma ** 2).sum()\n","plt.figure(figsize=(16,9))\n","plt.bar(range(len(Sigma)), explained_variance_ratio)\n","plt.ylabel('Ratio of variance explained')\n","plt.xlabel('principal component #')"]},{"cell_type":"markdown","metadata":{"id":"xxS8N9-JiAVU"},"source":["Hereafyer we plot the cumulative variance explained by the PCs. \n","\n","The Figure suggests that 84% of the variance in the data is already explained with the 1st component. With 4 PCs, more than 91% of the variance is explained. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:12:42.519592Z","iopub.status.busy":"2022-04-12T09:12:42.519107Z","iopub.status.idle":"2022-04-12T09:12:42.778268Z","shell.execute_reply":"2022-04-12T09:12:42.777681Z","shell.execute_reply.started":"2022-04-12T09:12:42.519538Z"},"id":"sT09Mm3XiIJe","outputId":"9b435605-3f6b-4576-db2a-e637fb4fee49"},"outputs":[],"source":["cumsum = np.cumsum(explained_variance_ratio)\n","plt.figure(figsize=(16,9))\n","plt.plot(range(len(explained_variance_ratio)), cumsum)\n","plt.ylabel('Cumulative sum of the explained variance')\n","plt.xlabel('# principal component')"]},{"cell_type":"markdown","metadata":{"id":"_dl429WRa3VP","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["# 2. Evaluation Metrics\n","## 2.0. Example: Accuracy\n","As example metric we take the accuracy. Informally, accuracy is the proportion of correct predictions over the total amount of predictions. It is used a lot in classification but it certainly has its disadvantages..."]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T10:03:08.217986Z","iopub.status.busy":"2022-04-12T10:03:08.217547Z","iopub.status.idle":"2022-04-12T10:03:08.222922Z","shell.execute_reply":"2022-04-12T10:03:08.222194Z","shell.execute_reply.started":"2022-04-12T10:03:08.217948Z"},"id":"8famA6Cka3VQ","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"OHp4BxQta3VQ","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["# 3. Classifiers\n","## 3.0. Example: The *'not so smart'* classifier\n","This random classifier is not very complicated. It makes predictions at random, based on the distribution obseved in the training set. **It thus assumes** that the class labels of the test set will be distributed similarly to the training set."]},{"cell_type":"markdown","metadata":{},"source":["Let us try a simple example with a random classifier"]},{"cell_type":"code","execution_count":320,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:56:53.966002Z","iopub.status.busy":"2022-04-12T12:56:53.965689Z","iopub.status.idle":"2022-04-12T12:56:54.226519Z","shell.execute_reply":"2022-04-12T12:56:54.225639Z","shell.execute_reply.started":"2022-04-12T12:56:53.965971Z"},"id":"SxWtPTSwgQKN","trusted":true},"outputs":[],"source":["# Calculate HOG parameters for test data set\n","from skimage.feature import hog\n","\n","# pixels = 8\n","hog_df_test=pd.DataFrame()\n","hog_df_test['img']=list(train_X_full)\n","fvs_test = [None]*len(train_X_full)\n","for i, img in enumerate(hog_df_test['img']):\n","    if i in face_ind:\n","        if gray_var:\n","            fvs_test[i] = list(hog(img, orientations=9, pixels_per_cell=(pixels,pixels), block_norm='L2',\n","                    cells_per_block=(2, 2), feature_vector='True'))\n","        else:\n","            fvs_test[i] = list(hog(img, orientations=9, pixels_per_cell=(pixels,pixels), block_norm='L2',\n","                    cells_per_block=(2, 2), feature_vector='True',channel_axis=channel))\n","hog_df_test['fv']= fvs_test\n","hog_train = np.asarray(fvs)"]},{"cell_type":"code","execution_count":321,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:56:54.831306Z","iopub.status.busy":"2022-04-12T12:56:54.830638Z","iopub.status.idle":"2022-04-12T12:56:54.835115Z","shell.execute_reply":"2022-04-12T12:56:54.834289Z","shell.execute_reply.started":"2022-04-12T12:56:54.831262Z"},"id":"NFlGN_fca3VQ","outputId":"64b10544-c8ae-4c21-e5ec-da1a40de93f6","trusted":true},"outputs":[],"source":["y_pred_hog_test = random_class.predict(hog_df_test['fv'])\n","for i, cl in enumerate(y_pred_hog_test):\n","    if cl == 3 or cl == 4:\n","        y_pred_hog_test[i] = 0\n","        \n","print(y_pred_hog_test)"]},{"cell_type":"code","execution_count":322,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:56:55.830530Z","iopub.status.busy":"2022-04-12T12:56:55.829819Z","iopub.status.idle":"2022-04-12T12:56:55.834340Z","shell.execute_reply":"2022-04-12T12:56:55.833472Z","shell.execute_reply.started":"2022-04-12T12:56:55.830487Z"},"id":"5ZRR_6SSa3VR","outputId":"823d9912-27ae-41a1-8218-a894f2196eb1","trusted":true},"outputs":[],"source":["# Calculate accuracy with test dataset\n","print(\"accuracy on train dataset: \")\n","print(accuracy_score(test_y_acc, y_pred_hog_test))"]},{"cell_type":"markdown","metadata":{},"source":["Let us use now a SVM classifier"]},{"cell_type":"code","execution_count":186,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:03:35.413293Z","iopub.status.busy":"2022-04-12T12:03:35.412851Z","iopub.status.idle":"2022-04-12T12:03:35.416495Z","shell.execute_reply":"2022-04-12T12:03:35.415928Z","shell.execute_reply.started":"2022-04-12T12:03:35.413262Z"},"id":"uTB712fJa3VR","trusted":true},"outputs":[],"source":["from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":220,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:24:55.017332Z","iopub.status.busy":"2022-04-12T12:24:55.017067Z","iopub.status.idle":"2022-04-12T12:24:55.035581Z","shell.execute_reply":"2022-04-12T12:24:55.034637Z","shell.execute_reply.started":"2022-04-12T12:24:55.017304Z"},"id":"vOvKEstJa3VR","trusted":true},"outputs":[],"source":["# Return a list of featurevectors\n","\n","def preprocess_(img, size, Gray):\n","    norm = 100\n","    gray = cv2.cvtColor(np.uint8(img), cv2.COLOR_RGB2GRAY)\n","    if Gray == False:\n","        gray = img\n","        norm = 255\n","        \n","    #gray = cv2.cvtColor(np.uint8(img), cv2.COLOR_RGB2GRAY)\n","    fvs = []\n","    \n","    #face\n","    faces = detector(gray, 1)\n","    for face in faces:\n","    \n","        (x, y, w, h) = face.left(), face.top(), face.width(), face.height()\n","        shape=predictor(gray,face)\n","\n","        #eyes \n","        #right eye, list of all landmarks, then compute center of mass\n","        re_xvalues = []\n","        re_yvalues=[]\n","        for i in range(36,42):\n","            re_xvalues.append(shape.part(i).x)\n","            re_yvalues.append(shape.part(i).y)\n","\n","        le_xvalues = []\n","        le_yvalues=[]\n","        for i in range(42,48):\n","            le_xvalues.append(shape.part(i).x)\n","            le_yvalues.append(shape.part(i).y)     \n","\n","        #center of mass\n","        left_eye = [np.mean(le_xvalues), np.mean(le_yvalues)]\n","        right_eye = [np.mean(re_xvalues), np.mean(re_yvalues)]\n","\n","        dX=left_eye[0]-right_eye[0]\n","        dY = left_eye[1]-right_eye[1]\n","        angle = np.arctan(dY/dX)\n","\n","        #calculate translation\n","        r_eye_loc = 1-l_eye_loc\n","\n","        dist = np.sqrt((dX ** 2) + (dY ** 2))\n","        desiredDist = r_eye_loc-l_eye_loc\n","        desiredDist *= 256\n","        scale = desiredDist / dist\n","\n","        eye_center = ((left_eye[0]+right_eye[0])//2 , (left_eye[1]+right_eye[1])//2)\n","\n","        M = cv2.getRotationMatrix2D(eye_center, math.degrees(angle), scale)\n","\n","        tX = 256 * 0.5\n","        tY = 256 * l_eye_loc\n","        M[0, 2] += (tX - eye_center[0])\n","        M[1, 2] += (tY - eye_center[1])\n","\n","        img_rotated = cv2.warpAffine(gray, M, (256, 256)) \n","\n","\n","        #normalize\n","        norm_img = np.zeros((size, size))\n","        norm_img = cv2.normalize(img_rotated, norm_img, 0, norm, cv2.NORM_MINMAX)\n","\n","        cropped_im= cv2.resize(norm_img,(size,size),interpolation=cv2.INTER_LINEAR )\n","#         plt.imshow(cropped_im)\n","#         plt.show()\n","        fv = list(hog(cropped_im, orientations=9, pixels_per_cell=(pixels,pixels), block_norm='L2',\n","                    cells_per_block=(2, 2), feature_vector='True', channel_axis=channel))\n","        \n","        fvs.append(fv) \n","        \n","    return fvs"]},{"cell_type":"markdown","metadata":{},"source":["Let us now do cross validation to find the best SVM parameters"]},{"cell_type":"code","execution_count":162,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T11:50:00.941059Z","iopub.status.busy":"2022-04-12T11:50:00.940510Z","iopub.status.idle":"2022-04-12T11:50:00.947656Z","shell.execute_reply":"2022-04-12T11:50:00.946649Z","shell.execute_reply.started":"2022-04-12T11:50:00.941021Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import cross_val_score"]},{"cell_type":"markdown","metadata":{},"source":["For the determination we use a simple cross-validation technique."]},{"cell_type":"code","execution_count":190,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:03:49.462748Z","iopub.status.busy":"2022-04-12T12:03:49.462479Z","iopub.status.idle":"2022-04-12T12:05:52.442366Z","shell.execute_reply":"2022-04-12T12:05:52.441376Z","shell.execute_reply.started":"2022-04-12T12:03:49.462720Z"},"trusted":true},"outputs":[],"source":["for C_ in [1, 5, 10]:\n","    for gamma_ in [0.0001, 0.001, 0.01, 0.1]:\n","        print(\"C= %0.1f, gamma= %0.4f\"% (C_, gamma_))\n","        clf = SVC(kernel='rbf', C=C_, gamma=gamma_, probability=True)# parameters have been optimized for this training set.\n","        scores = cross_val_score(clf, hog_train, train_y_full_trans, cv=6)\n","        print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"]},{"cell_type":"markdown","metadata":{},"source":["The best parameters found are 10 = 5 and gamma = 0.01"]},{"cell_type":"code","execution_count":324,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T12:57:08.062021Z","iopub.status.busy":"2022-04-12T12:57:08.061729Z","iopub.status.idle":"2022-04-12T12:57:08.127963Z","shell.execute_reply":"2022-04-12T12:57:08.127197Z","shell.execute_reply.started":"2022-04-12T12:57:08.061993Z"},"id":"gNLMigcwa3VR","trusted":true},"outputs":[],"source":["# Test SVM classifier\n","clf = SVC(kernel='rbf', C=10, gamma=0.01, probability=True)# parameters have been optimized for this training set.\n","clf = clf.fit(hog_train, train_y_full)"]},{"cell_type":"markdown","metadata":{},"source":["To classify the test set each image undergoes a couple of if-statements.\n","Remember that in the preprocessing part we kept track of the images on which there was not a single face recognized. Based on that we make our first if statement.\n","\n","if #faces == 1:\n","    calculate probabilities of each label using our SVM-classifier, if the highest one         among those is higher than the threshold we assign that label to it.\n","    If it is lower than the threshold we assume that it is a random face so it will be a       zero\n","if #faces > 1:\n","    do the above for each face and pick the highest prob among those faces.\n","    However, if one of the probabilities of label 1 or 2 surpasses the threshold we assign     the corresponding label. This is for images on which there is an 1 and 0.\n","if #faces == 0:\n","    assign 0"]},{"cell_type":"code","execution_count":332,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T13:00:29.854664Z","iopub.status.busy":"2022-04-12T13:00:29.854333Z","iopub.status.idle":"2022-04-12T13:00:32.351030Z","shell.execute_reply":"2022-04-12T13:00:32.350309Z","shell.execute_reply.started":"2022-04-12T13:00:29.854627Z"},"id":"DayDTBuKa3VS","outputId":"ce50cc94-bce8-465e-e000-4faf1ca51e76","trusted":true},"outputs":[],"source":["# Find labels for test dataset\n","threshold = .85\n","\n","for i,img in enumerate(test_X):\n","    if i in face_ind:\n","        fv = list(hog(img, orientations=9, pixels_per_cell=(pixels,pixels), block_norm='L2',\n","                    cells_per_block=(2, 2), feature_vector='True',channel_axis=channel))\n","        \n","        prob = clf.predict_proba([fv])\n","        if max(prob[0])>threshold:\n","            label = np.argmax(prob)\n","            if label==2 or label==3:\n","                label = 0\n","                labels[i] = label\n","            else:\n","                label+=1\n","                labels[i] = label\n","        else:\n","            label = 0\n","            labels[i] = 0\n","    elif i in faces_ind:\n","        fv_faces = preprocess_(test['img'][i], size, Gray = gray_var)\n","        probs = clf.predict_proba(fv_faces)\n","        highest_prob = threshold\n","        label_ = 0\n","        \n","        # Return face with highest probability\n","        for p in probs:\n","            if max(p)>highest_prob:\n","                highest_prob = max(p)\n","                label = np.argmax(p)\n","                if np.argmax(p)==0 or np.argmax(p)==1:\n","                    break\n","        if label == 2 or label == 3:\n","            label=0\n","            labels[i] = label\n","        else:\n","            label +=1\n","            labels[i] = label\n","        if i==7:\n","            \n","            plt.imshow(test['img'][i])\n","            plt.show()\n","            print(\"Probabilities of Detected Faces:\")\n","            print(probs)\n","            print(\"Chosen Label For Picture:\")\n","            print(label)\n","print(accuracy_score(labels,test_y_acc))"]},{"cell_type":"markdown","metadata":{},"source":["Let us print the result classes"]},{"cell_type":"markdown","metadata":{},"source":["Let us calculate the accuracy"]},{"cell_type":"code","execution_count":333,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T13:00:37.639112Z","iopub.status.busy":"2022-04-12T13:00:37.638608Z","iopub.status.idle":"2022-04-12T13:00:37.644496Z","shell.execute_reply":"2022-04-12T13:00:37.643890Z","shell.execute_reply.started":"2022-04-12T13:00:37.639064Z"},"trusted":true},"outputs":[],"source":["print(accuracy_score(labels,test_y_acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-04-12T09:14:30.019782Z","iopub.status.busy":"2022-04-12T09:14:30.019498Z","iopub.status.idle":"2022-04-12T09:14:33.665612Z","shell.execute_reply":"2022-04-12T09:14:33.663930Z","shell.execute_reply.started":"2022-04-12T09:14:30.019754Z"},"id":"JrfT1jEOa3VS","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["# Print the results\n","\n","for i, image in enumerate(test_X):\n","     plt.imshow(image)\n","     plt.show()\n","    \n","     if labels[i]==1:\n","         print(\"Jesse\")\n","     elif labels[i]==2:\n","         print(\"Mila\")\n","     else:\n","         print(\"Michael or Sara\")\n","    "]},{"cell_type":"markdown","metadata":{},"source":["Let us do the same for the PCA features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:21:16.783102Z","iopub.status.busy":"2022-04-12T09:21:16.782797Z","iopub.status.idle":"2022-04-12T09:21:16.850313Z","shell.execute_reply":"2022-04-12T09:21:16.849142Z","shell.execute_reply.started":"2022-04-12T09:21:16.783072Z"}},"outputs":[],"source":["clf_pca = SVC(kernel='rbf', C=10, gamma=0.000001, probability=True)# parameters have been optimized for this training set.\n","scores = cross_val_score(clf_pca, pca_train.principal_components, train_y_full_trans, cv=5)\n","\n","print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"]},{"cell_type":"markdown","metadata":{},"source":["0.37 is best that we can found for PCA with C = 10 and gamma = 0.01"]},{"cell_type":"markdown","metadata":{},"source":["This is due to this data augmentation that we have done which does not work well with PCA (an idea would to replace the bright images by saturate images)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:21:19.961384Z","iopub.status.busy":"2022-04-12T09:21:19.961111Z","iopub.status.idle":"2022-04-12T09:21:19.985526Z","shell.execute_reply":"2022-04-12T09:21:19.984640Z","shell.execute_reply.started":"2022-04-12T09:21:19.961355Z"}},"outputs":[],"source":["# Test SVM classifier\n","clf_pca = SVC(kernel='rbf', C=10, gamma=0.01, probability=True)# parameters have been optimized for this training set.\n","clf_pca = clf_pca.fit(pca_train.principal_components, train_y_full_trans)"]},{"cell_type":"markdown","metadata":{"id":"EyJlsiCwa3VT"},"source":["As we notice that HOG works better than PCA, let us quickly calculate the accuracy with the following functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-12T09:21:39.350813Z","iopub.status.busy":"2022-04-12T09:21:39.350163Z","iopub.status.idle":"2022-04-12T09:21:39.357554Z","shell.execute_reply":"2022-04-12T09:21:39.356936Z","shell.execute_reply.started":"2022-04-12T09:21:39.350777Z"}},"outputs":[],"source":["y_pred_pca_test = clf_pca.predict(pca_test.principal_components)\n","# Calculate accuracy with test dataset\n","print(\"accuracy on train dataset: \")\n","print(accuracy_score(y_pred_pca_test,test_y_acc))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
